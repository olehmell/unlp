{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":89576,"databundleVersionId":10931344,"sourceType":"competition"},{"sourceId":11182909,"sourceType":"datasetVersion","datasetId":6977146}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"id\":\"5OH-Bgo4NSK5_modified_imports\"}\n# Install necessary libraries if not already present (especially for Colab)\n!pip install pymystem3 imbalanced-learn # or latest versions\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport ast  # For safely evaluating string representations of lists\nimport os # For file paths\n\n# Preprocessing & Feature Engineering\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom pymystem3 import Mystem # For Lemmatization\n# from sklearn.preprocessing import FunctionTransformer # For Trigger words (Optional)\n# from scipy.sparse import hstack # For combining features (Optional)\n\n# Modeling & Evaluation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_recall_curve, hamming_loss\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom imblearn.over_sampling import SMOTE # For SMOTE\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# For reproducibility\nnp.random.seed(42)\n\n# --- Configuration ---\n# Mount Google Drive (if your data is stored there)\nUSE_DRIVE = False # Set to False if data is local\nBASE_DRIVE_PATH = \"/content/drive/MyDrive/For Colab/\" # Adjust this path as needed!\n\nif USE_DRIVE:\n    try:\n        from google.colab import drive\n        drive.mount('/content/drive')\n        DATA_PATH_TRAIN_N = os.path.join(BASE_DRIVE_PATH, \"train.parquet\")\n        DATA_PATH_TRAIN_S = os.path.join(BASE_DRIVE_PATH, \"synthetic_train_dataset.csv\")\n        DATA_PATH_TEST = os.path.join(BASE_DRIVE_PATH, \"test.csv\") # Path for final test file\n        SUBMISSION_PATH = \"/content/submission.csv\" # Output path for submission\n        print(\"Google Drive mounted.\")\n    except ModuleNotFoundError:\n        print(\"Not running in Colab or Drive mounting failed. Assuming data is local.\")\n        USE_DRIVE = False # Fallback to local if drive fails\n        # Define local paths if USE_DRIVE=False or mounting fails\n        DATA_PATH_TRAIN_N = \"train.parquet\"\n        DATA_PATH_TRAIN_S = \"synthetic_train_dataset.csv\"\n        DATA_PATH_TEST = \"/kaggle/input/unlp-2025-shared-task-classification-techniques/test.csv\"\n        SUBMISSION_PATH = \"/kaggle/working/submission-tf-idf.csv\"\nelse:\n     # Define local paths if USE_DRIVE=False from the start\n    DATA_PATH_TRAIN_N = \"/kaggle/input/unlp-2025-shared-task-classification-techniques/train.parquet\"\n    DATA_PATH_TRAIN_S = \"/kaggle/input/unlp-dataset/synthetic_train_dataset.csv\"\n    DATA_PATH_TEST = \"/kaggle/input/unlp-2025-shared-task-classification-techniques/test.csv\"\n    SUBMISSION_PATH = \"/kaggle/working/submission-tf-idf.csv\"\n\n\nprint(f\"Using Native Train data path: {DATA_PATH_TRAIN_N}\")\nprint(f\"Using Synthetic Train data path: {DATA_PATH_TRAIN_S}\")\nprint(f\"Using Test data path: {DATA_PATH_TEST}\")\nprint(f\"Using Submission output path: {SUBMISSION_PATH}\")\n\n# Initialize Lemmatizer (Mystem) - outside function for efficiency\ntry:\n    lemmatizer = Mystem()\n    print(\"Mystem lemmatizer initialized.\")\nexcept Exception as e:\n    print(f\"Could not initialize Mystem: {e}. Lemmatization will be skipped.\")\n    lemmatizer = None","metadata":{"_uuid":"15cc0cda-1a14-48a0-86f2-78b36603f4e4","_cell_guid":"63fb151c-795d-4f8a-be0f-cb3276850c99","trusted":true,"collapsed":false,"id":"5OH-Bgo4NSK5","outputId":"388671e0-a546-42ac-cd43-18c028e91f78","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] {\"id\":\"xmlvyMbNOQpw\",\"outputId\":\"6f4a8783-1e1f-4a52-fa73-4083915acddd\"}\n# Load the dataset\ntry:\n    df = pd.read_parquet(DATA_PATH_TRAIN_N)\n    print(f\"Loaded {DATA_PATH_TRAIN_N} successfully.\")\n    # try:\n    #     df_s = pd.read_csv(DATA_PATH_TRAIN_S)\n    #     print(f\"Loaded {DATA_PATH_TRAIN_S} successfully.\")\n    #     df = pd.concat([df_n, df_s], ignore_index=True) # Use ignore_index for clean concat\n    #     print(\"Concatenated native and synthetic datasets.\")\n    # except FileNotFoundError:\n    #     print(f\"Warning: Synthetic data file not found at {DATA_PATH_TRAIN_S}. Using only native data.\")\n    #     df = df_n\n    # except Exception as e_s:\n    #     print(f\"Warning: Error loading synthetic data from {DATA_PATH_TRAIN_S}: {e_s}. Using only native data.\")\n    #     df = df_n\n\nexcept FileNotFoundError:\n    print(f\"Error: Native data file not found at {DATA_PATH_TRAIN_N}\")\n    print(\"Please ensure the file exists and the path is correct.\")\n    raise SystemExit(\"Stopping notebook execution - Native training data required.\")\nexcept Exception as e_n:\n    print(f\"Error loading native data from {DATA_PATH_TRAIN_N}: {e_n}\")\n    raise SystemExit(\"Stopping notebook execution.\")\n\n\n# Display basic info and first few rows\nprint(\"\\nDataset Info:\")\ndf.info()\nprint(\"\\nFirst 5 rows:\")\nprint(df.head())\nprint(f\"\\nDataset shape: {df.shape}\")\n\n# --- Revised Handling of Missing Values ---\n# 1. Drop rows ONLY if 'content' is missing, as text is essential.\nprint(\"\\nMissing values before handling:\")\nprint(df.isnull().sum())\ndf.dropna(subset=['content'], inplace=True)\nprint(f\"\\nShape after dropping rows with missing 'content': {df.shape}\")\n\n# 2. Fill missing 'techniques' with an empty string representation '[]'\ndf['techniques'] = df['techniques'].fillna('[]').astype(str)\n\n# Check again (should show 0 nulls for content and techniques)\nprint(\"\\nMissing values after handling:\")\nprint(df.isnull().sum())\n\nprint(f\"\\nDataset shape after handling NaNs: {df.shape}\")","metadata":{"_uuid":"93f60bbb-99a4-4153-baa0-df947dfe2683","_cell_guid":"ed2f1821-7846-49ea-8999-7f8498f99567","trusted":true,"collapsed":false,"id":"xmlvyMbNOQpw","outputId":"6f4a8783-1e1f-4a52-fa73-4083915acddd","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Cell 3: Parse Techniques","metadata":{"_uuid":"8e811538-666c-404b-934b-0fb4b4a56c62","_cell_guid":"e1ca7f3f-07e0-4996-90ce-aa5061b4ddaf","trusted":true,"collapsed":false,"id":"bKudQfE6QokL","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# %% [markdown] {\"id\":\"bKudQfE6QokL\"}\n# Cell 3: Parse Techniques\n\n# %% [code] {\"id\":\"thv6SSpAOX0h\",\"outputId\":\"a4a4a6da-9b73-4c96-ad30-7b34d51592c3\"}\n# --- Define the Canonical List of Techniques ---\n# These are the ONLY techniques we want to classify\nCANONICAL_TECHNIQUES = sorted([\n    'straw_man', 'appeal_to_fear', 'fud', 'bandwagon', 'whataboutism',\n    'loaded_language', 'glittering_generalities', 'euphoria',\n    'cherry_picking', 'cliche'\n])\nprint(f\"Target Techniques ({len(CANONICAL_TECHNIQUES)}):\")\nprint(CANONICAL_TECHNIQUES)\n# --------------------------------------------\n\n# Function to safely parse the 'techniques' string, filter by canonical list,\n# AND handle concatenated techniques.\ndef parse_filter_split_techniques(tech_string):\n    # 1. Handle None or obvious empty representations first\n    if tech_string is None or not isinstance(tech_string, str) or tech_string.strip().lower() in ['', '[]', '{}', '()', 'nan']:\n        return []\n\n    # Use a set to automatically handle duplicates\n    found_techniques = set()\n    processed_string = tech_string.strip().lower() # Work with lowercase, stripped string\n\n    # 2. Try standard parsing (ast.literal_eval) for clean list formats\n    try:\n        evaluated = ast.literal_eval(processed_string)\n        if isinstance(evaluated, list):\n            for item in evaluated:\n                item_clean = str(item).strip()\n                if item_clean in CANONICAL_TECHNIQUES:\n                    found_techniques.add(item_clean)\n                # --- Substring Check WITHIN list items ---\n                # Check if this item itself is a concatenation\n                elif item_clean: # Avoid check on empty strings from list\n                        for canonical_tech in CANONICAL_TECHNIQUES:\n                            if canonical_tech in item_clean:\n                                found_techniques.add(canonical_tech)\n            # If ast worked, we assume it covers the structure, return results\n            return sorted(list(found_techniques))\n\n    except (ValueError, SyntaxError):\n        # ast failed, proceed to regex and substring checks on the original string\n        pass\n\n    # 3. Try regex for potentially separated words (if ast failed)\n    # This helps with formats like \"'tech1' 'tech2'\" or just single words\n    techniques_found_regex = re.findall(r\"\\'(.*?)\\'|\\\"(.*?)\\\"|(\\b\\w+(?:[-']\\w+)*\\b)\", processed_string)\n    flat_list = [item.strip() for sublist in techniques_found_regex for item in sublist if item and item.strip()]\n\n    # If regex found multiple distinct items OR the only item found matches a canonical technique, add them\n    if len(flat_list) > 1 or (len(flat_list) == 1 and flat_list[0] in CANONICAL_TECHNIQUES):\n        for tech in flat_list:\n            if tech in CANONICAL_TECHNIQUES:\n                found_techniques.add(tech)\n\n    # 4. Perform Substring Check on the original (cleaned) string\n    # This is the main part to catch concatenations like 'loaded_languagewhataboutism'\n    # It runs even if regex found something, to be safe.\n    # We clean potential list artifacts just in case\n    cleaned_for_substring = processed_string.replace(\"'\", \"\").replace('\"', '').replace('[', '').replace(']', '').replace(',', ' ').strip()\n    # Remove extra spaces that might result from replacement\n    cleaned_for_substring = re.sub(r'\\s+', ' ', cleaned_for_substring).strip()\n\n    if cleaned_for_substring: # Avoid checking empty strings\n        for canonical_tech in CANONICAL_TECHNIQUES:\n            # Check if the canonical tech exists as a whole word or part of the string\n            # Simple 'in' check is more robust for direct concatenation\n            if canonical_tech in cleaned_for_substring:\n                found_techniques.add(canonical_tech)\n\n    # 5. Return the unique, filtered, sorted list\n    return sorted(list(found_techniques))\n\n# --- Apply the NEW parsing function ---\ndf['parsed_techniques_filtered'] = df['techniques'].apply(parse_filter_split_techniques)\n\n# --- The rest of Cell 3 remains the same ---\n\n# Verify the parsing and filtering - check examples, especially concatenated ones\nprint(\"\\nSample of original, parsed & filtered techniques:\")\nprint(df[['techniques', 'parsed_techniques_filtered']].head(10))\nprint(df[['techniques', 'parsed_techniques_filtered']].tail(10)) # Check end as well\n\n# -- MultiLabelBinarizer --\n# Initialize explicitly with the canonical classes\nmlb = MultiLabelBinarizer(classes=CANONICAL_TECHNIQUES)\n\n# Fit and transform using the filtered techniques column\ny = mlb.fit_transform(df['parsed_techniques_filtered'])\n\n# Get the names of the technique columns (will match CANONICAL_TECHNIQUES now)\ntechnique_columns = mlb.classes_ # Should be the same as CANONICAL_TECHNIQUES now\nprint(f\"\\nShape of binary label matrix y: {y.shape}\")\nprint(f\"Columns in y correspond to: {technique_columns}\")\n\n# Create a DataFrame with the binary labels\ny_df = pd.DataFrame(y, columns=technique_columns, index=df.index)\n\n# Concatenate with the original dataframe (optional, but useful for inspection)\ndf_processed = pd.concat([df, y_df], axis=1)\n\nprint(\"\\nDataFrame with binary technique columns (first 5 rows):\")\nprint(df_processed[['content'] + list(technique_columns)].head())\n\n# Check class distribution for each technique\nprint(\"\\nClass Distribution (Number of positive samples per technique):\")\nprint(y_df.sum().sort_values(ascending=False))\n\n# Check how many samples have NO techniques (all zeros in y_df)\nno_technique_count = (y_df.sum(axis=1) == 0).sum()\nprint(f\"\\nNumber of samples with NO techniques identified (among the {len(technique_columns)} target ones): {no_technique_count}\")","metadata":{"_uuid":"cb7aab87-ccd5-4d63-bc0b-cf641b8a0576","_cell_guid":"c7e9091b-980b-4332-a349-02d6b08d2f3c","trusted":true,"collapsed":false,"id":"thv6SSpAOX0h","outputId":"a4a4a6da-9b73-4c96-ad30-7b34d51592c3","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] {\"id\":\"l9u-hPBZPJAM_modified_clean_triggers_numpy\"}\nimport numpy as np # Ensure numpy is imported\nimport pandas as pd # Ensure pandas is imported\n\n# Basic text cleaning function WITH LEMMATIZATION (remains the same)\ndef clean_text(text):\n    if not isinstance(text, str): return \"\"\n    text = text.lower()\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\@\\w+|\\#', '', text)\n    text = re.sub(r'[^\\w\\s\\']', ' ', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    if lemmatizer:\n        try:\n            lemmas = lemmatizer.lemmatize(text)\n            text = \"\".join(lemmas).strip()\n            text = re.sub(r'\\s+', ' ', text).strip()\n        except Exception as e:\n            # print(f\"Warning: Lemmatization failed for text snippet: '{text[:50]}...' Error: {e}\")\n            pass\n    return text\n\n\n# --- Apply Cleaning and Feature Engineering ---\n\n# 1. Apply cleaning to the 'content' column\nprint(\"\\nApplying text cleaning and lemmatization...\")\ndf_processed['cleaned_content'] = df_processed['content'].apply(clean_text)\nprint(\"Text cleaning and lemmatization complete.\")\n\nprint(\"\\nSample of original vs cleaned/lemmatized content:\")\nprint(df_processed[['content', 'cleaned_content']].head())","metadata":{"_uuid":"96135a75-5ead-480b-8425-b7e8008f2aa4","_cell_guid":"fc01a70f-a468-487a-b90a-8402aad9a660","trusted":true,"collapsed":false,"id":"l9u-hPBZPJAM","outputId":"5512fa12-96ef-4f11-fc9f-27055b6d7089","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# --- Helper function to extract phrases for a single row ---\n# Returns a list of phrases, or None if no valid phrases can be extracted.\ndef extract_trigger_phrases_for_row(row):\n    \"\"\"Extracts text phrases based on spans in 'trigger_words' for a single row.\"\"\"\n    # Make sure 'content' is treated as a string\n    content = str(row['content']) if pd.notna(row['content']) else \"\"\n    trigger_obj = row['trigger_words']\n\n    spans_to_process = trigger_obj\n\n    # Check if spans were actually found or if array/list was empty/invalid\n    if not isinstance(spans_to_process, np.ndarray) and not spans_to_process: # Handles empty list\n         if isinstance(trigger_obj, np.ndarray) and trigger_obj.size == 0: # Handles empty numpy array\n              return None\n         # Optionally warn about unexpected formats, but might be too verbose\n         return None # No valid spans found or unexpected format\n    # Extract phrases using spans\n    extracted_phrases = []\n    content_len = len(content)\n    for span in spans_to_process:\n        try:\n            start = int(span[0])\n            end = int(span[1])\n            # Validate indices: 0 <= start < end <= content_length\n            if 0 <= start < content_len and start < end and end <= content_len:\n                phrase = content[start:end].strip()\n\n                if phrase:\n                    # 2. Clean the extracted phrase using the main cleaning function\n                    cleaned_phrase = clean_text(phrase)\n                    # 3. Add the cleaned phrase ONLY if it's not empty after cleaning\n                    if cleaned_phrase:\n                        extracted_phrases.append(cleaned_phrase)\n        except (ValueError, TypeError, IndexError):\n            continue # Skip invalid spans\n\n    return extracted_phrases if extracted_phrases else None # Return list of phrases or None\n\n\n\nprint(df_processed.columns)\n\n\n# 2. Initialize the global dictionary using defaultdict\n# Use dictionary comprehension to initialize with empty lists\ntechnique_triggers_global = {tech: [] for tech in CANONICAL_TECHNIQUES}\nprint(f\"Initialized 'technique_triggers_global' with {len(technique_triggers_global)} techniques (all starting with empty lists).\")\n\n# 3. Process trigger words and aggregate into the global dictionary\nif 'trigger_words' in df_processed.columns and 'parsed_techniques_filtered' in df_processed.columns:\n    print(\"\\nExtracting trigger phrases and aggregating into global dictionary 'technique_triggers_global'...\")\n\n    processed_rows = 0\n    phrases_aggregated = 0\n    # Iterate through the DataFrame rows\n    for index, row in df_processed.iterrows():\n        # Ensure 'parsed_techniques_filtered' is a list\n        techniques_list = row['parsed_techniques_filtered'] if isinstance(row['parsed_techniques_filtered'], list) else []\n\n        # Skip rows with no techniques identified\n        if not techniques_list:\n            continue\n\n        # Extract phrases for the current row\n        extracted_phrases = extract_trigger_phrases_for_row(row)\n\n        # If phrases were extracted, add them to the global dict for each relevant technique\n        if extracted_phrases: # Check if the list is not None and not empty\n            for tech in techniques_list:\n                # defaultdict automatically creates the list for a new key\n                technique_triggers_global[tech].extend(extracted_phrases)\n                phrases_aggregated += len(extracted_phrases) # Count total phrases added\n            processed_rows += 1\n\n    print(f\"Finished aggregation. Processed {processed_rows} rows with techniques and triggers.\")\n    print(f\"Aggregated a total of {phrases_aggregated} phrase instances across all techniques.\")\n\n    # Display info about the created dictionary\n    if technique_triggers_global:\n        print(f\"\\nTechniques found in dictionary: {len(technique_triggers_global)}\")\n        print(\"Number of trigger phrases aggregated per technique (top 10):\")\n        # Calculate counts and sort\n        phrase_counts = {tech: len(phrases) for tech, phrases in technique_triggers_global.items()}\n        sorted_counts = sorted(phrase_counts.items(), key=lambda item: item[1], reverse=True)\n        for tech, count in sorted_counts[:10]:\n            print(f\"- {tech}: {count}\")\n\n        # Example: Print a few phrases for one technique\n        # sample_tech = sorted_counts[0][0] if sorted_counts else None # Get tech with most phrases\n        # if sample_tech:\n        #    print(f\"\\nSample phrases for '{sample_tech}' (first 5 unique):\")\n        #    unique_phrases = sorted(list(set(technique_triggers_global[sample_tech])))\n        #    print(unique_phrases[:5])\n    else:\n        print(\"The global dictionary 'technique_triggers_global' is empty.\")\n\n\nelse:\n    print(\"\\nSkipping global technique-trigger phrase aggregation: 'trigger_words' or 'parsed_techniques_filtered' column not found.\")\n    # technique_triggers_global remains an empty defaultdict","metadata":{"_uuid":"96135a75-5ead-480b-8425-b7e8008f2aa4","_cell_guid":"fc01a70f-a468-487a-b90a-8402aad9a660","trusted":true,"collapsed":false,"id":"l9u-hPBZPJAM","outputId":"5512fa12-96ef-4f11-fc9f-27055b6d7089","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] {\"id\":\"eaUdI_TwPacR_train_val_test_split\"}\n# Define features (X) and target labels (y_df created via MLB in Cell 2/3)\nX = df_processed['cleaned_content']\n# y_df holds the multi-label binary targets\n\n# --- Split Data into Training, Validation, and Test Sets ---\n# First split into Train+Validation (e.g., 80%) and Test (20%)\nX_train_val, X_test, y_train_val_df, y_test_df = train_test_split(\n    X,\n    y_df,\n    test_size=0.20, # 20% for the final test set\n    random_state=42\n    # Stratification is complex for multi-label; not used here but consider if needed.\n)\n\n# Split the Train+Validation set into actual Training (e.g., 75% of 80% = 60% total)\n# and Validation (e.g., 25% of 80% = 20% total)\nX_train, X_val, y_train_df, y_val_df = train_test_split(\n    X_train_val,\n    y_train_val_df,\n    test_size=0.25, # 0.25 * 0.80 = 0.20 (20% of total for validation)\n    random_state=42 # Using same random state for reproducibility within this split\n)\n\nprint(f\"Training set shape: X={X_train.shape}, y={y_train_df.shape}\")\nprint(f\"Validation set shape: X={X_val.shape}, y={y_val_df.shape}\") # Added Validation set\nprint(f\"Test set shape: X={X_test.shape}, y={y_test_df.shape}\")\n\n# Verify the split percentages\ntotal_len = len(X_train) + len(X_val) + len(X_test)\nprint(f\"\\nTotal samples: {len(X)}. Split samples: {total_len}\")\nprint(f\"Train %: {len(X_train)/len(X):.2f}, Val %: {len(X_val)/len(X):.2f}, Test %: {len(X_test)/len(X):.2f}\")","metadata":{"_uuid":"9c8cf3eb-69ef-462a-af79-0010510fc4ad","_cell_guid":"2755dedc-677f-4945-bbe2-d2089c290963","trusted":true,"collapsed":false,"id":"eaUdI_TwPacR","outputId":"59953e02-d456-43dc-9a79-94b23d38173e","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] {\"id\":\"ldZU4pxwPi-a_tfidf_combine_val\"}\nimport re\nfrom scipy.sparse import hstack, csr_matrix\nimport numpy as np\n\n# Initialize TF-IDF Vectorizer (same as before)\ntfidf_vectorizer = TfidfVectorizer(\n    max_features=10000,\n    analyzer='char_wb',\n    ngram_range=(3, 5),\n    min_df=3,\n    max_df=0.9\n)\nprint(\"Initializing TF-IDF Vectorizer...\")\nprint(\"Fitting TF-IDF on training data...\")\ntfidf_vectorizer.fit(X_train) # Fit only on X_train\nprint(\"Fit complete.\")\nprint(\"Transforming datasets (TF-IDF)...\")\nX_train_tfidf = tfidf_vectorizer.transform(X_train)\nX_val_tfidf = tfidf_vectorizer.transform(X_val) # Transform validation set\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\nprint(\"TF-IDF transformation complete.\")\nprint(f\"Shape of TF-IDF matrices: Train={X_train_tfidf.shape}, Val={X_val_tfidf.shape}, Test={X_test_tfidf.shape}\") # Added Val shape\n\n# --- Add Trigger Phrase Presence Features ---\n\nprint(\"\\nAttempting to add Trigger Phrase Presence features...\")\nUSE_TRIGGER_PRESENCE_FEATURES = False # Default flag\n\nif 'technique_triggers_global' not in globals() or not isinstance(technique_triggers_global, dict):\n    print(\"Warning: 'technique_triggers_global' dictionary not found or invalid. Skipping trigger presence features.\")\nelse:\n    print(f\"Found 'technique_triggers_global' with {len(technique_triggers_global)} techniques.\")\n    trigger_patterns = {}\n    # ...(logic to compile trigger_patterns - same as before)...\n    for tech, phrases in technique_triggers_global.items():\n        valid_phrases = [re.escape(p) for p in phrases if len(p) > 1]\n        if valid_phrases:\n            pattern = r'\\b(' + '|'.join(valid_phrases) + r')\\b'\n            try: trigger_patterns[tech] = re.compile(pattern, flags=re.IGNORECASE)\n            except Exception as e_re: print(f\"Warning: Error compiling regex for '{tech}'. Skipping. Error: {e_re}\")\n\n    print(f\"Compiled regex patterns for {len(trigger_patterns)} techniques.\")\n\n    def check_trigger_presence_regex(text, pattern): # Function remains the same\n        if pattern and isinstance(text, str): return 1 if pattern.search(text) else 0\n        return 0\n\n    trigger_features_train = []\n    trigger_features_val = [] # List for validation features\n    trigger_features_test = []\n    feature_names = []\n\n    if 'CANONICAL_TECHNIQUES' not in globals(): raise NameError(\"CANONICAL_TECHNIQUES list not found.\")\n\n    print(\"Generating trigger presence features for Train/Val/Test sets...\") # Added Val\n    for tech in CANONICAL_TECHNIQUES:\n        pattern = trigger_patterns.get(tech)\n        if pattern:\n            feature_name = f'trigger_present_{tech}'\n            feature_names.append(feature_name)\n            trigger_features_train.append(X_train.apply(lambda text: check_trigger_presence_regex(text, pattern)))\n            trigger_features_val.append(X_val.apply(lambda text: check_trigger_presence_regex(text, pattern))) # Apply to Val\n            trigger_features_test.append(X_test.apply(lambda text: check_trigger_presence_regex(text, pattern)))\n\n    if trigger_features_train:\n        print(f\"Created {len(feature_names)} trigger presence features.\")\n        trigger_array_train = np.array(trigger_features_train).T\n        trigger_array_val = np.array(trigger_features_val).T # Create array for Val\n        trigger_array_test = np.array(trigger_features_test).T\n\n        trigger_sparse_train = csr_matrix(trigger_array_train)\n        trigger_sparse_val = csr_matrix(trigger_array_val) # Create sparse for Val\n        trigger_sparse_test = csr_matrix(trigger_array_test)\n\n        print(\"Combining TF-IDF features with trigger presence features...\")\n        X_train_combined = hstack([X_train_tfidf, trigger_sparse_train], format='csr')\n        X_val_combined = hstack([X_val_tfidf, trigger_sparse_val], format='csr') # Combine for Val\n        X_test_combined = hstack([X_test_tfidf, trigger_sparse_test], format='csr')\n\n        X_train_tfidf = X_train_combined\n        X_val_tfidf = X_val_combined # Update Val variable\n        X_test_tfidf = X_test_combined\n\n        print(f\"Shape AFTER combining: Train={X_train_tfidf.shape}, Val={X_val_tfidf.shape}, Test={X_test_tfidf.shape}\") # Added Val shape\n        USE_TRIGGER_PRESENCE_FEATURES = True\n    else:\n        print(\"No trigger presence features created. Using only TF-IDF.\")\n        USE_TRIGGER_PRESENCE_FEATURES = False\n# --- End of Trigger Feature Addition ---","metadata":{"_uuid":"b82a5f80-114f-490e-8ff2-3a7d002e1f4a","_cell_guid":"8a1960df-2b02-4d27-8ed2-595e54046989","trusted":true,"collapsed":false,"id":"ldZU4pxwPi-a","outputId":"a08cd2e0-8fd2-4012-f546-38942fc5b3d2","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] {\"id\":\"50MEBVX3Plnn_reinstate_tuning\"}\nfrom sklearn.metrics import f1_score, precision_recall_curve # Need PRC for tuning\n\n# Dictionary to store trained models, BEST thresholds, and evaluation results\nmodels = {}\nbest_thresholds = {} # Store the best threshold found for each technique\nresults = {}\n\n# Loop through each technique column defined in CANONICAL_TECHNIQUES\nfor technique in technique_columns:\n    print(f\"\\n--- Processing: {technique} ---\")\n\n    # Get the specific target labels for this technique for all splits\n    y_train_technique = y_train_df[technique]\n    y_val_technique = y_val_df[technique] # Use validation labels\n    y_test_technique = y_test_df[technique]\n\n    # Check class distribution in training data\n    positive_class_count_train = y_train_technique.sum()\n    total_count_train = len(y_train_technique)\n    print(f\"Training data: {positive_class_count_train} positive samples out of {total_count_train}\")\n    if positive_class_count_train < 5: print(f\"Warning: Very few positive samples for '{technique}' in TRAINING data.\")\n\n    # --- Apply SMOTE (remains the same) ---\n    X_train_resampled = X_train_tfidf\n    y_train_resampled = y_train_technique\n    try: # SMOTE Logic\n        if positive_class_count_train > 0 and positive_class_count_train < total_count_train:\n            print(\"Applying SMOTE...\")\n            k_neighbors_smote = min(4, positive_class_count_train - 1) if positive_class_count_train > 1 else 1\n            smote = SMOTE(random_state=42, k_neighbors=k_neighbors_smote)\n            X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train_technique)\n            print(f\"After SMOTE: {y_train_resampled.sum()}/{len(y_train_resampled)}\")\n        else: print(\"Skipping SMOTE.\")\n    except Exception as e_smote: print(f\"SMOTE Error for {technique}: {e_smote}. Using original data.\")\n\n    # Initialize the classifier (remains the same)\n    model = LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42, C=1.0)\n\n    # Train the model (on potentially resampled training data)\n    print(\"Training model...\")\n    model.fit(X_train_resampled, y_train_resampled)\n    print(\"Training complete.\")\n\n    # --- Threshold Tuning using VALIDATION set ---\n    print(\"Performing threshold tuning on validation set...\")\n    # Predict probabilities on the VALIDATION set\n    y_val_pred_proba = model.predict_proba(X_val_tfidf)[:, 1] # Probabilities for the positive class\n\n    best_val_f1 = -1\n    current_best_thresh = 0.5 # Default threshold\n\n    # Check if validation set has both classes needed for F1 score\n    if len(np.unique(y_val_technique)) == 2:\n        # Method 1: Iterate through thresholds (simpler)\n        thresholds_to_try = np.arange(0.1, 0.91, 0.05) # Explore thresholds 0.1 to 0.9\n        f1_scores_val = []\n        for thresh in thresholds_to_try:\n            y_val_pred_tuned = (y_val_pred_proba >= thresh).astype(int)\n            f1 = f1_score(y_val_technique, y_val_pred_tuned, average='binary', pos_label=1, zero_division=0)\n            f1_scores_val.append(f1)\n            if f1 > best_val_f1:\n                best_val_f1 = f1\n                current_best_thresh = thresh\n\n        # Method 2: Use Precision-Recall curve (more robust, finds threshold near optimal point)\n        # precision, recall, thresholds_prc = precision_recall_curve(y_val_technique, y_val_pred_proba)\n        # # Calculate F1 score for each threshold from PRC, excluding the last dummy threshold\n        # f1_scores_prc = (2 * precision * recall) / (precision + recall + 1e-9) # Add epsilon for division stability\n        # best_prc_f1_idx = np.argmax(f1_scores_prc[:-1]) # Find index of best F1\n        # best_prc_f1 = f1_scores_prc[best_prc_f1_idx]\n        # # Check if PRC method found a better threshold\n        # if best_prc_f1 > best_val_f1 :\n        #      current_best_thresh = thresholds_prc[best_prc_f1_idx]\n        #      best_val_f1 = best_prc_f1\n        #      print(f\"(Using PRC threshold: {current_best_thresh:.4f})\")\n\n        print(f\"Best threshold found: {current_best_thresh:.2f} with Validation F1: {best_val_f1:.4f}\")\n\n    else:\n        print(f\"Warning: Validation set for '{technique}' has only one class. Cannot perform F1-based tuning. Using default 0.5.\")\n        current_best_thresh = 0.5 # Fallback to default\n\n    # Store the best threshold found for this technique\n    best_thresholds[technique] = current_best_thresh\n\n    # --- Evaluation on TEST set using the BEST threshold found on Validation ---\n    print(f\"Evaluating on TEST set using threshold {current_best_thresh:.2f}...\")\n    # Predict probabilities on the TEST set\n    y_test_pred_proba = model.predict_proba(X_test_tfidf)[:, 1]\n    # Apply the TUNED threshold\n    y_test_pred_final = (y_test_pred_proba >= current_best_thresh).astype(int)\n\n    # --- Calculate Test Metrics (same logic as before, but uses tuned predictions) ---\n    accuracy = accuracy_score(y_test_technique, y_test_pred_final)\n    print(f\"Test Accuracy: {accuracy:.4f}\")\n    test_f1 = 0.0\n    report = \"N/A - Only one class present in y_test\"\n    cm = None\n    unique_classes_in_test = np.unique(y_test_technique)\n    num_unique_classes_test = len(unique_classes_in_test)\n\n    if num_unique_classes_test == 2:\n        test_f1 = f1_score(y_test_technique, y_test_pred_final, average='binary', pos_label=1, zero_division=0)\n        report = classification_report(y_test_technique, y_test_pred_final, target_names=['Not_' + technique, technique], zero_division=0)\n        cm = confusion_matrix(y_test_technique, y_test_pred_final, labels=[0, 1])\n        print(f\"Test F1-Score (Positive Class): {test_f1:.4f}\")\n        print(\"Test Classification Report:\")\n        print(report)\n        print(\"Test Confusion Matrix:\")\n        print(cm)\n        # Plot Confusion Matrix (remains the same)\n        plt.figure(figsize=(4, 3)); sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Pred Neg', 'Pred Pos'], yticklabels=['True Neg', 'True Pos']); plt.title(f'CM: {technique} (Test)'); plt.show()\n    else: print(f\"Warning: Only one class ({unique_classes_in_test}) found in TEST set for '{technique}'.\")\n\n    # Store the model and results\n    models[technique] = model\n    results[technique] = {\n        'accuracy': accuracy,\n        'f1_score_positive': test_f1 if num_unique_classes_test == 2 else np.nan,\n        'best_threshold': current_best_thresh, # Store the threshold used\n        'report': report, 'confusion_matrix': cm\n    }\n\nprint(\"\\n--- Processing Complete for all techniques ---\")\nprint(\"\\nBest thresholds found (tuned on validation set):\")\n# Print thresholds sorted by technique name for consistency\nfor tech in sorted(best_thresholds.keys()):\n    print(f\"- {tech}: {best_thresholds[tech]:.4f}\")","metadata":{"_uuid":"a34a35eb-a34d-499a-8933-41268d592a90","_cell_guid":"d5443257-d6db-40ba-8d28-991edf4a19e7","trusted":true,"collapsed":false,"id":"50MEBVX3Plnn","outputId":"dc257c20-8990-4278-c7c1-7680fa6b5568","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] {\"id\":\"VhZ9l8kySD3W_metrics_tuned\"}\n# --- Part 1: Summary of Individual Classifier Performance ---\nsummary_df = pd.DataFrame(results).T\n# Include best_threshold in summary\nsummary_df = summary_df[['accuracy', 'f1_score_positive', 'best_threshold']].astype({'f1_score_positive': float, 'accuracy': float, 'best_threshold': float})\nprint(\"\\n--- Individual Classifier Performance Summary (Test Set w/ Tuned Thresholds) ---\") # Updated print\nprint(summary_df.sort_values(by='f1_score_positive', ascending=False))\n\n# --- Part 2: Calculate Overall Multi-Label Metrics on TEST Set ---\nprint(\"\\n--- Overall Multi-Label Performance Metrics (Test Set w/ Tuned Thresholds) ---\") # Updated print\n\n# Ensure best_thresholds dictionary exists from Cell 6\nif 'best_thresholds' not in globals():\n     raise NameError(\"The 'best_thresholds' dictionary was not found. Please ensure Cell 6 (Training/Tuning) ran correctly.\")\n\n# 1. Generate the multi-label prediction matrix for the TEST set using TUNED thresholds\nall_test_predictions = {}\nfor technique, model in models.items():\n    if model is not None:\n        # Get probabilities for the positive class on the test set\n        y_pred_proba_single_test = model.predict_proba(X_test_tfidf)[:, 1] # X_test_tfidf might be combined\n        # Apply the specific BEST threshold found for this technique\n        threshold = best_thresholds.get(technique, 0.5) # Use stored threshold, default if missing (shouldn't happen if Cell 6 ran)\n        y_pred_single_test = (y_pred_proba_single_test >= threshold).astype(int)\n        all_test_predictions[technique] = y_pred_single_test\n    else:\n        print(f\"Warning: Model for '{technique}' not found. Predicting zeros.\")\n        all_test_predictions[technique] = np.zeros(X_test_tfidf.shape[0], dtype=int)\n\ny_pred_multi_df = pd.DataFrame(all_test_predictions, index=y_test_df.index)[technique_columns]\ny_true_multi_test = y_test_df.values\ny_pred_multi_test = y_pred_multi_df.values\n\n# 2. Calculate Metrics (logic remains the same)\n# ... (calculation of macro_f1, micro_f1, etc. - same as before) ...\nmacro_f1 = f1_score(y_true_multi_test, y_pred_multi_test, average='macro', zero_division=0)\nprint(f\"\\nOverall Macro-F1 Score (Test): {macro_f1:.4f}\")\nmicro_f1 = f1_score(y_true_multi_test, y_pred_multi_test, average='micro', zero_division=0)\nprint(f\"Overall Micro-F1 Score (Test): {micro_f1:.4f}\")\nweighted_f1 = f1_score(y_true_multi_test, y_pred_multi_test, average='weighted', zero_division=0)\nprint(f\"Overall Weighted-F1 Score (Test): {weighted_f1:.4f}\")\nsamples_f1 = f1_score(y_true_multi_test, y_pred_multi_test, average='samples', zero_division=0)\nprint(f\"Overall Samples-F1 Score (Test): {samples_f1:.4f}\")\nhamming = hamming_loss(y_true_multi_test, y_pred_multi_test)\nprint(f\"Hamming Loss (Test - Lower is better): {hamming:.4f}\")\n\n# 3. Display the full multi-label classification report (logic remains the same)\nprint(\"\\nOverall Classification Report (Multi-Label - Test Set w/ Tuned Thresholds):\") # Updated print\nprint(classification_report(y_true_multi_test, y_pred_multi_test, target_names=technique_columns, zero_division=0))","metadata":{"_uuid":"0c3780a5-f5f0-4a37-9f79-32e921079f4f","_cell_guid":"5e4f4f91-6158-4c7b-8006-c8ba4f7ae662","trusted":true,"collapsed":false,"id":"VhZ9l8kySD3W","outputId":"2be199b9-99b9-44fe-afe8-194f9355ae4d","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] {\"id\":\"GQziKOCYT8uT_submission_tuned\"}\nimport re\nfrom scipy.sparse import hstack, csr_matrix\nimport numpy as np\n\n# --- Prerequisites Check ---\n# Ensure best_thresholds is checked\nrequired_vars = ['models', 'tfidf_vectorizer', 'technique_columns', 'best_thresholds', # Added best_thresholds back\n                 'X_train_tfidf', 'y_train_df', 'clean_text', 'trigger_patterns',\n                 'USE_TRIGGER_PRESENCE_FEATURES', 'CANONICAL_TECHNIQUES',\n                 'DATA_PATH_TEST', 'SUBMISSION_PATH']\nmissing_vars = [var for var in required_vars if var not in locals() and var not in globals()]\nif missing_vars: raise NameError(f\"Missing required variables: {missing_vars}.\")\n\nprint(\"--- Part 1: Evaluate Model Fit on Training Data (Using Tuned Thresholds) ---\") # Updated print\n# 1. Generate predictions for the TRAINING set using TUNED thresholds\nall_train_predictions = {}\nprint(\"Generating predictions on the training data using tuned thresholds...\") # Updated print\nfor technique, model in models.items():\n    if model is not None:\n        y_pred_proba_single_train = model.predict_proba(X_train_tfidf)[:, 1] # X_train_tfidf might be combined\n        threshold = best_thresholds.get(technique, 0.5) # Use tuned threshold\n        y_pred_single_train = (y_pred_proba_single_train >= threshold).astype(int)\n        all_train_predictions[technique] = y_pred_single_train\n    else:\n        print(f\"Warning: Model for '{technique}' not found. Predicting zeros.\")\n        all_train_predictions[technique] = np.zeros(X_train_tfidf.shape[0], dtype=int)\n# ... (rest of training evaluation calculation and printing - same as before) ...\ny_pred_train_multi_df = pd.DataFrame(all_train_predictions, index=y_train_df.index)[technique_columns]\ny_true_train_multi = y_train_df.values\ny_pred_train_multi = y_pred_train_multi_df.values\nprint(\"\\nCalculating multi-label metrics on TRAINING data...\")\n# ... (Calculate and print training metrics - same calculation logic) ...\ntrain_macro_f1 = f1_score(y_true_train_multi, y_pred_train_multi, average='macro', zero_division=0); print(f\"Training Set Macro-F1: {train_macro_f1:.4f}\")\ntrain_micro_f1 = f1_score(y_true_train_multi, y_pred_train_multi, average='micro', zero_division=0); print(f\"Training Set Micro-F1: {train_micro_f1:.4f}\")\ntrain_samples_f1 = f1_score(y_true_train_multi, y_pred_train_multi, average='samples', zero_division=0); print(f\"Training Set Samples-F1: {train_samples_f1:.4f}\")\ntrain_hamming = hamming_loss(y_true_train_multi, y_pred_train_multi); print(f\"Training Set Hamming Loss: {train_hamming:.4f}\")\nprint(\"\\nTraining Set Classification Report (Multi-Label):\")\nprint(classification_report(y_true_train_multi, y_pred_train_multi, target_names=technique_columns, zero_division=0))\nprint(\"-\" * 50)\n\n\nprint(\"\\n--- Part 2: Load and Predict on Official Test Data (test.csv) ---\")\n# 1. Load Official Test Data\n# ... (loading logic - same as before) ...\ntry: df_official_test = pd.read_csv(DATA_PATH_TEST)\nexcept FileNotFoundError: raise SystemExit(\"Stopping - Official Test file required.\")\nexcept Exception as e: raise SystemExit(f\"Stopping - Failed to load official test file: {e}\")\nif 'id' not in df_official_test.columns: raise ValueError(\"Column 'id' not found.\")\nif 'content' not in df_official_test.columns: raise ValueError(\"Column 'content' not found.\")\nprint(f\"Loaded {DATA_PATH_TEST}. Shape: {df_official_test.shape}\")\n\n# 2. Preprocess Official Test Data Text\n# ... (preprocessing logic - same as before) ...\nprint(\"\\nPreprocessing official test text...\")\ndf_official_test['content'] = df_official_test['content'].fillna('')\ndf_official_test['cleaned_content'] = df_official_test['content'].apply(clean_text)\nprint(\"Preprocessing complete.\")\n\n# 3. Transform Official Test Data using Fitted TF-IDF Vectorizer\n# ... (TF-IDF transform logic - same as before) ...\nprint(\"\\nApplying TF-IDF transformation...\")\ntry: X_official_test_tfidf = tfidf_vectorizer.transform(df_official_test['cleaned_content'])\nexcept Exception as e: raise RuntimeError(f\"Error during TF-IDF transformation: {e}\")\nprint(f\"TF-IDF transformation complete. Shape: {X_official_test_tfidf.shape}\")\n\n# --- Apply Trigger Presence Features to Official Test Data (if used in training) ---\n# (Logic remains the same as previous version, combining if USE_TRIGGER_PRESENCE_FEATURES is True)\nif USE_TRIGGER_PRESENCE_FEATURES:\n    print(\"\\nApplying trigger presence features to official test data...\")\n    def check_trigger_presence_regex(text, pattern): # Function needs to be available\n        if pattern and isinstance(text, str): return 1 if pattern.search(text) else 0\n        return 0\n    trigger_features_official_test = []\n    for tech in CANONICAL_TECHNIQUES:\n        pattern = trigger_patterns.get(tech)\n        if pattern:\n            trigger_features_official_test.append(df_official_test['cleaned_content'].apply(lambda text: check_trigger_presence_regex(text, pattern)))\n    if trigger_features_official_test:\n        trigger_array_official_test = np.array(trigger_features_official_test).T\n        trigger_sparse_official_test = csr_matrix(trigger_array_official_test)\n        print(\"Combining TF-IDF with trigger features...\")\n        X_official_test_tfidf = hstack([X_official_test_tfidf, trigger_sparse_official_test], format='csr')\n        print(f\"Shape AFTER combining: {X_official_test_tfidf.shape}\")\n    else: print(\"Warning: USE_TRIGGER_PRESENCE_FEATURES was True, but no trigger features generated.\")\n\n# 4. Predict on Transformed Official Test Data using TUNED Thresholds\nprint(\"\\nGenerating predictions for official test data using tuned thresholds...\") # Updated print\nofficial_test_predictions = {}\nfor technique, model in models.items():\n    if model is not None:\n        # Predict probabilities\n        y_pred_proba_official_test = model.predict_proba(X_official_test_tfidf)[:, 1] # X_official_test_tfidf might be combined\n        # Apply the TUNED threshold for this technique\n        threshold = best_thresholds.get(technique, 0.5) # Use stored threshold\n        y_pred_official_test_single = (y_pred_proba_official_test >= threshold).astype(int)\n        official_test_predictions[technique] = y_pred_official_test_single\n    else:\n        print(f\"Warning: Model for '{technique}' not found. Predicting zeros.\")\n        official_test_predictions[technique] = np.zeros(X_official_test_tfidf.shape[0], dtype=int)\n\ndf_official_test_predictions = pd.DataFrame(official_test_predictions)[technique_columns]\nprint(\"Predictions generated.\")\n\n\nprint(\"\\n--- Part 3: Create Submission File ---\")\n# (This part remains the same)\n# ... (Combine IDs and predictions) ...\n# ... (Verify column order) ...\n# ... (Save to CSV) ...\ndf_submission = pd.concat([df_official_test[['id']], df_official_test_predictions], axis=1)\nexpected_columns = ['id'] + list(technique_columns)\nif list(df_submission.columns) != expected_columns: print(\"Warning: Column order mismatch. Reordering.\"); df_submission = df_submission[expected_columns]\nprint(f\"\\nSubmission DataFrame preview (first 5 rows):\"); print(df_submission.head())\ntry: df_submission.to_csv(SUBMISSION_PATH, index=False); print(f\"\\nSubmission file created: {SUBMISSION_PATH}\")\nexcept Exception as e: print(f\"\\nError saving submission file: {e}\")\n\nprint(\"\\n--- Cell 8 Execution Complete ---\")","metadata":{"_uuid":"76c632b2-c7ec-401d-a82a-652b541553ce","_cell_guid":"9771cb7c-88d1-4102-bcdf-5f8a067cc760","trusted":true,"collapsed":false,"id":"GQziKOCYT8uT","outputId":"cae004a2-ac94-458d-86a3-080ed48728fe","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"_uuid":"fde9e43f-d253-4fcd-b3c3-a92df28cdd46","_cell_guid":"9a44f847-e9b1-4a4c-a8a2-68a3efd08e1e","trusted":true,"collapsed":false,"id":"e0HyOZKLzrAo","jupyter":{"outputs_hidden":false}}}]}