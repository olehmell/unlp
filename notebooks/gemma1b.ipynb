{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e4f283eed8f6487ea8b872157021673d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84ee8cb137c94ccd8a88aa2069d2c37c",
              "IPY_MODEL_5bfe3f8e3a524ae1bc86b4a843d12cee",
              "IPY_MODEL_e4dd0665f009465cafb3db7f75707c97"
            ],
            "layout": "IPY_MODEL_f11a1b2a0d2b4394baa38b20facd16cd"
          }
        },
        "84ee8cb137c94ccd8a88aa2069d2c37c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54ab05ab0572469c8cf4f78db6d3404b",
            "placeholder": "​",
            "style": "IPY_MODEL_98017d1ac98f45dca6b16703e3345715",
            "value": "Map (num_proc=6): 100%"
          }
        },
        "5bfe3f8e3a524ae1bc86b4a843d12cee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ef0b5ee587e401aa4b53ccdced3eb0e",
            "max": 3823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d600bb8dcd640c7a7d8e070ac170662",
            "value": 3823
          }
        },
        "e4dd0665f009465cafb3db7f75707c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c72ca6f3402e40ff868ea7d33b748b7c",
            "placeholder": "​",
            "style": "IPY_MODEL_aedcbe29400047e1b7461c853303d7ce",
            "value": " 3823/3823 [00:00&lt;00:00, 2663.88 examples/s]"
          }
        },
        "f11a1b2a0d2b4394baa38b20facd16cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54ab05ab0572469c8cf4f78db6d3404b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98017d1ac98f45dca6b16703e3345715": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ef0b5ee587e401aa4b53ccdced3eb0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d600bb8dcd640c7a7d8e070ac170662": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c72ca6f3402e40ff868ea7d33b748b7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aedcbe29400047e1b7461c853303d7ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6905d6a1ba774038b6a2a8b0d4e16a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2bda5fc2d4541f69224c6976fdcd85f",
              "IPY_MODEL_95b52542d5d64678956aa3227013079a",
              "IPY_MODEL_e6676d6927a648f5b192fdd0acc53829"
            ],
            "layout": "IPY_MODEL_adfeb6e583a54abbbed553d8adda0aa4"
          }
        },
        "b2bda5fc2d4541f69224c6976fdcd85f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_126ae3927e3a425d8ebe906d9bdcae5d",
            "placeholder": "​",
            "style": "IPY_MODEL_29f80c18eb144fd3a70220679874f568",
            "value": "Filter (num_proc=6): 100%"
          }
        },
        "95b52542d5d64678956aa3227013079a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f4cc5e08c9d4ff4bff80ad93541cd77",
            "max": 3823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e53fcd85df794216993a23c7f20624a3",
            "value": 3823
          }
        },
        "e6676d6927a648f5b192fdd0acc53829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1aef4810ac294872bb213be6b4993f67",
            "placeholder": "​",
            "style": "IPY_MODEL_15d5bf78b3224ffb8800d3f04d65e899",
            "value": " 3823/3823 [00:00&lt;00:00, 5048.59 examples/s]"
          }
        },
        "adfeb6e583a54abbbed553d8adda0aa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "126ae3927e3a425d8ebe906d9bdcae5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29f80c18eb144fd3a70220679874f568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f4cc5e08c9d4ff4bff80ad93541cd77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e53fcd85df794216993a23c7f20624a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1aef4810ac294872bb213be6b4993f67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15d5bf78b3224ffb8800d3f04d65e899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b8c68fb4cea40f5a0de14004acfeb90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7cd9a7bc1657408eaa01fa8c815af623",
              "IPY_MODEL_eeecdbda0df448068010a1ba402ae227",
              "IPY_MODEL_691ea887f70646f38b8f47f6b4573c67"
            ],
            "layout": "IPY_MODEL_0a25d922f665423184c3adbf457bd255"
          }
        },
        "7cd9a7bc1657408eaa01fa8c815af623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1c3fc73a3a7493d8f68858a938d331e",
            "placeholder": "​",
            "style": "IPY_MODEL_456f2ff223784bba998b7b8baab56e62",
            "value": "Map (num_proc=6): 100%"
          }
        },
        "eeecdbda0df448068010a1ba402ae227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d23657962aac4a0d96198e353027511b",
            "max": 3823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98dcff9f8eed46d0ac1952029904249f",
            "value": 3823
          }
        },
        "691ea887f70646f38b8f47f6b4573c67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7235f3499fb94b6bbf2e1a3c3d9c65f7",
            "placeholder": "​",
            "style": "IPY_MODEL_5d36b0b7b7524f12bfee382e2fc9a605",
            "value": " 3823/3823 [00:00&lt;00:00, 12760.93 examples/s]"
          }
        },
        "0a25d922f665423184c3adbf457bd255": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1c3fc73a3a7493d8f68858a938d331e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "456f2ff223784bba998b7b8baab56e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d23657962aac4a0d96198e353027511b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98dcff9f8eed46d0ac1952029904249f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7235f3499fb94b6bbf2e1a3c3d9c65f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d36b0b7b7524f12bfee382e2fc9a605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e28c54efa8d406c8950a4bef739a038": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_368cf1ec5b69410caea669cb94040665",
              "IPY_MODEL_19283cefd2c14e7cac80d4ee445549aa",
              "IPY_MODEL_02aa9335e2224bc1b99367396f38e9b2"
            ],
            "layout": "IPY_MODEL_d3405363ea224ac7b47389ae3ae5b8f5"
          }
        },
        "368cf1ec5b69410caea669cb94040665": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a498914516e4ed49c91a8aeb6f4ad4e",
            "placeholder": "​",
            "style": "IPY_MODEL_ef4d040c546243d6a36593d181cebeb5",
            "value": "Filter (num_proc=6): 100%"
          }
        },
        "19283cefd2c14e7cac80d4ee445549aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d50eb6cadb4e4af5ad0c10e36b65ff16",
            "max": 3823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5c01aea6ccb4c719bcca3ca49f15fb0",
            "value": 3823
          }
        },
        "02aa9335e2224bc1b99367396f38e9b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6786c44e026547f9835462fe207be687",
            "placeholder": "​",
            "style": "IPY_MODEL_f5f4b3b04e6c4066a15a71d5c02e8993",
            "value": " 3823/3823 [00:00&lt;00:00, 4907.38 examples/s]"
          }
        },
        "d3405363ea224ac7b47389ae3ae5b8f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a498914516e4ed49c91a8aeb6f4ad4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef4d040c546243d6a36593d181cebeb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d50eb6cadb4e4af5ad0c10e36b65ff16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5c01aea6ccb4c719bcca3ca49f15fb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6786c44e026547f9835462fe207be687": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5f4b3b04e6c4066a15a71d5c02e8993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PwwgxWkbTG3",
        "outputId": "811c196b-dd3a-4d41-e1d6-2b54770db5b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Colab - installing dependencies.\n",
            "Requirement already satisfied: unsloth>=2024.7 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-newest]>=2024.7) (2025.3.19)\n",
            "Requirement already satisfied: transformers>=4.49.0 in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: xformers==0.0.29.post3 in /usr/local/lib/python3.11/dist-packages (0.0.29.post3)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: trl<0.9.0 in /usr/local/lib/python3.11/dist-packages (0.8.6)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
            "Requirement already satisfied: unsloth_zoo in /usr/local/lib/python3.11/dist-packages (2025.3.17)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.29.3)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (0.1.9)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Colab installations complete.\n",
            "GPU detected: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# %%capture\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# --- Installations ---\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Check if running in Colab or locally\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Running in Colab - installing dependencies.\")\n",
        "    # Install necessary libraries for Colab\n",
        "    # Unsloth for efficient fine-tuning\n",
        "    !pip install --no-deps \"unsloth[colab-newest]>=2024.7\"\n",
        "    # vLLM (optional, might have compatibility issues in Colab)\n",
        "    # !pip install \"vllm<0.6.0\"\n",
        "    # Latest Transformers for Gemma-3 support\n",
        "    !pip install --no-deps \"transformers>=4.49.0\"\n",
        "    # Other required libraries\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl<0.9.0\" triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer pandas # Added pandas\n",
        "    print(\"Colab installations complete.\")\n",
        "else:\n",
        "    print(\"Running locally.\")\n",
        "    # Check if essential libraries are installed, prompt user if not\n",
        "    try:\n",
        "        import unsloth\n",
        "        import transformers\n",
        "        import datasets\n",
        "        import torch\n",
        "        import pandas\n",
        "        import peft\n",
        "        import trl\n",
        "        print(\"Required libraries seem to be installed.\")\n",
        "    except ImportError as e:\n",
        "        print(f\"Missing library: {e.name}\")\n",
        "        print(\"Please ensure unsloth, transformers, datasets, torch, pandas, peft, trl are installed.\")\n",
        "        print(\"Example: pip install unsloth transformers datasets torch pandas peft trl bitsandbytes accelerate sentencepiece protobuf huggingface_hub hf_transfer\")\n",
        "        # Optionally, uncomment the line below to attempt installation locally (use with caution)\n",
        "        # !pip install unsloth transformers datasets torch pandas peft trl bitsandbytes accelerate sentencepiece protobuf huggingface_hub hf_transfer\n",
        "\n",
        "# Verify GPU availability\n",
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"WARNING: No GPU detected. Training and inference will be very slow.\")\n",
        "else:\n",
        "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Core Libraries ---\n",
        "from unsloth import FastModel\n",
        "\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset # Added Dataset for dummy data fallback\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "from peft import PeftModel\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Check if running in Colab again for specific logic if needed later\n",
        "IN_COLAB = 'google.colab' in sys.modules"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-hM7i_6bU-U",
        "outputId": "6b696d7b-fb9d-4b12-ddc1-57974315b784"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "\n",
        "# Labels (use submission column names)\n",
        "# Ensure these exactly match the required columns in submission.csv (except 'id')\n",
        "SUBMISSION_LABELS = sorted([\n",
        "    'straw_man', 'appeal_to_fear', 'fud', 'bandwagon', 'whataboutism',\n",
        "    'loaded_language', 'glittering_generalities', 'euphoria',\n",
        "    'cherry_picking', 'cliche'\n",
        "])\n",
        "\n",
        "# Mapping from your training data labels to SUBMISSION_LABELS\n",
        "# Add/modify mappings based on *your* specific train.csv 'techniques' values\n",
        "TRAINING_LABEL_MAP = {\n",
        "    'loaded_language': 'loaded_language',\n",
        "    'glittering_generalities': 'glittering_generalities',\n",
        "    'euphoria': 'euphoria',\n",
        "    'appeal_to_fear': 'appeal_to_fear',\n",
        "    'fud': 'fud',\n",
        "    'fear_uncertainty_doubt': 'fud', # Example variation\n",
        "    'bandwagon': 'bandwagon',\n",
        "    'appeal_to_people': 'bandwagon',\n",
        "    'thought_terminating_cliche': 'cliche',\n",
        "    'cliche': 'cliche',\n",
        "    'whataboutism': 'whataboutism',\n",
        "    'cherry_picking': 'cherry_picking',\n",
        "    'straw_man': 'straw_man',\n",
        "    # Add more mappings here if needed...\n",
        "}\n",
        "\n",
        "# Model Parameters\n",
        "model_name = \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\" # Or choose another model\n",
        "max_seq_length = 2048  # Adjust based on GPU memory and typical text length\n",
        "load_in_4bit = False  # Set to True to use 4-bit quantization\n",
        "load_in_8bit = False  # Set to True to use 8-bit quantization (ignored if 4-bit is True)\n",
        "\n",
        "# File Paths (!!! ADJUST THESE PATHS !!!)\n",
        "train_csv_path = '/content/drive/MyDrive/For Colab/train.csv' # Path to your training CSV file\n",
        "test_csv_path = '/content/drive/MyDrive/For Colab/test.csv'   # Path to your test CSV file\n",
        "output_dir = \"manipulation_classifier_finetune\" # Directory for saving checkpoints and adapters\n",
        "submission_path = \"submission.csv\" # Name for the output submission file\n",
        "\n",
        "# LoRA Parameters\n",
        "lora_r = 8\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.05\n",
        "lora_target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                       \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "\n",
        "# Start by trying a larger batch size\n",
        "batch_size = 64           # TRY THIS FIRST (adjust based on VRAM usage)\n",
        "gradient_accumulation = 2 # Reduce accumulation (Effective batch size = 128)\n",
        "warmup_steps = 10\n",
        "num_epochs = 14\n",
        "learning_rate = 2e-5\n",
        "logging_steps = 10\n",
        "# save_strategy = \"epoch\" # Option 1: Save only once per epoch\n",
        "save_strategy = \"steps\"   # Option 2: Save every N steps\n",
        "save_steps = 50          # Increase if using save_strategy=\"steps\"\n",
        "seed = 3407\n",
        "\n",
        "# --- Derived Configuration ---\n",
        "# Determine compute dtype based on GPU capability\n",
        "compute_dtype = None\n",
        "if torch.cuda.is_available():\n",
        "    if torch.cuda.is_bf16_supported():\n",
        "        compute_dtype = torch.bfloat16\n",
        "        print(\"Using bfloat16\")\n",
        "    else:\n",
        "        compute_dtype = torch.float16\n",
        "        print(\"Using float16\")\n",
        "else:\n",
        "    # CPU or unsupported GPU\n",
        "    compute_dtype = torch.float32\n",
        "    print(\"GPU not available or doesn't support float16/bfloat16. Using float32 (warning: slow).\")\n",
        "\n",
        "\n",
        "print(f\"Target labels for classification: {SUBMISSION_LABELS}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldUnLaFQbYG8",
        "outputId": "787e9c40-1818-4904-97f2-8e0a63f39283"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using bfloat16\n",
            "Target labels for classification: ['appeal_to_fear', 'bandwagon', 'cherry_picking', 'cliche', 'euphoria', 'fud', 'glittering_generalities', 'loaded_language', 'straw_man', 'whataboutism']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Model and Tokenizer ---\n",
        "print(f\"Loading model: {model_name}\")\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=compute_dtype,  # Set dtype based on detected capability\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    load_in_8bit=load_in_8bit,\n",
        "    # token = \"hf_...\", # Add Hugging Face token if using gated models like Llama\n",
        ")\n",
        "print(\"Model and tokenizer loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ug-3itIBbeGQ",
        "outputId": "99579a1b-3076-47f5-9890-3c0a6d180417"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: unsloth/gemma-3-1b-it-unsloth-bnb-4bit\n",
            "==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.50.0.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n",
            "Model and tokenizer loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Add LoRA Adapters ---\n",
        "print(\"Applying LoRA adapters...\")\n",
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r=lora_r,\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    bias=\"none\",\n",
        "    target_modules=lora_target_modules,\n",
        "    random_state=seed,\n",
        ")\n",
        "print(\"LoRA adapters applied.\")\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_YsBN1Kbera",
        "outputId": "94304f84-bac6-418a-f399-f5d86c16b442"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying LoRA adapters...\n",
            "Unsloth: Making `model.base_model.model.model` require gradients\n",
            "LoRA adapters applied.\n",
            "trainable params: 6,522,880 || all params: 1,006,408,832 || trainable%: 0.6481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this dictionary, likely in Cell 3 (Configuration & Label Definitions)\n",
        "LABEL_DESCRIPTIONS = {\n",
        "    'appeal_to_fear': \"The misuse of fear (often based on stereotypes or prejudices) to support a particular proposal.\",\n",
        "    'bandwagon': \"An attempt to persuade the audience to join and take action because “others are doing the same thing.”\",\n",
        "    'cherry_picking': \"Selective use of data or facts that support a hypothesis while ignoring counterarguments.\",\n",
        "    'cliche': \"Commonly used phrases that mitigate cognitive dissonance and block critical thinking (e.g., “Everything is not so clear-cut”).\",\n",
        "    'euphoria': \"Using an event that causes euphoria or a feeling of happiness, or a positive event to boost morale, often used to mobilize the population.\",\n",
        "    'fud': \"Presenting information in a way that sows uncertainty and doubt, causing fear. A subtype of Appeal to Fear.\",\n",
        "    'glittering_generalities': \"Exploitation of people's positive attitude towards abstract concepts such as “justice,” “freedom,” “democracy,” “patriotism,” “peace,” etc., to provoke strong emotional reactions without specific information.\",\n",
        "    'loaded_language': \"The use of words and phrases with a strong emotional connotation (positive or negative) to influence the audience.\",\n",
        "    'straw_man': \"Distorting the opponent's position by replacing it with a weaker or outwardly similar one and refuting it instead.\",\n",
        "    'whataboutism': \"Discrediting the opponent's position by accusing them of hypocrisy without directly refuting their arguments.\"\n",
        "}\n",
        "\n",
        "# CELL 6: Data Preparation - Helper Functions (Multi-Label Revised)\n",
        "\n",
        "# --- Data Formatting Logic ---\n",
        "\n",
        "# Create the detailed labels string including descriptions\n",
        "# Ensure labels in SUBMISSION_LABELS have corresponding descriptions in LABEL_DESCRIPTIONS\n",
        "detailed_labels_list = []\n",
        "for label in SUBMISSION_LABELS:\n",
        "    description = LABEL_DESCRIPTIONS.get(label, \"No description available.\") # Fallback\n",
        "    detailed_labels_list.append(f\"- **{label}**: {description}\")\n",
        "labels_string_with_descriptions = \"\\n\".join(detailed_labels_list)\n",
        "\n",
        "# Updated instruction template asking for ALL applicable techniques\n",
        "instruction_prompt_template = \"\"\"Identify ALL applicable manipulation techniques used in the following text.\n",
        "Choose from the list below. If multiple techniques apply, list them separated by commas.\n",
        "\n",
        "Available Techniques:\n",
        "{labels_string_with_descriptions}\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\n",
        "Applicable Manipulation Technique(s):\"\"\" # Changed the final line label\n",
        "\n",
        "\n",
        "# Regex to extract labels like 'euphoria' from \"['euphoria' 'loaded_language']\"\n",
        "technique_pattern = re.compile(r\"'(.*?)'\")\n",
        "\n",
        "# Function to parse the techniques string and get ALL valid mapped labels\n",
        "def get_all_valid_labels(techniques_str):\n",
        "    \"\"\"\n",
        "    Parses the techniques string (e.g., \"['label1' 'label2']\")\n",
        "    and returns a sorted list of all labels found that map to a SUBMISSION_LABEL.\n",
        "    \"\"\"\n",
        "    valid_labels = set() # Use a set to avoid duplicates initially\n",
        "    if not isinstance(techniques_str, str):\n",
        "        return [] # Return empty list\n",
        "\n",
        "    potential_labels = technique_pattern.findall(techniques_str)\n",
        "\n",
        "    for train_label in potential_labels:\n",
        "        train_label_clean = train_label.strip().lower() # Normalize\n",
        "        mapped_label = TRAINING_LABEL_MAP.get(train_label_clean)\n",
        "        if mapped_label in SUBMISSION_LABELS:\n",
        "            valid_labels.add(mapped_label) # Add valid label to set\n",
        "\n",
        "    return sorted(list(valid_labels)) # Return sorted list\n",
        "\n",
        "# Function to format examples for SFTTrainer (using 'messages' format)\n",
        "def format_data_for_sft(example):\n",
        "    \"\"\"\n",
        "    Formats a single example for SFT. Uses 'content' for text and extracts\n",
        "    ALL valid labels from 'techniques', formatting them as a comma-separated string.\n",
        "    Returns {'messages': None} if the example should be skipped.\n",
        "    \"\"\"\n",
        "    text_content = example.get('content')\n",
        "    techniques_str = example.get('techniques')\n",
        "\n",
        "    user_content = instruction_prompt_template.format(\n",
        "        labels_string_with_descriptions=labels_string_with_descriptions,\n",
        "        text=text_content\n",
        "    )\n",
        "\n",
        "    if not techniques_str:\n",
        "        # Return {'messages': None} for schema consistency\n",
        "        return {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": user_content},\n",
        "                {\"role\": \"assistant\", \"content\": \"\"} # Model learns this string\n",
        "            ]\n",
        "        }\n",
        "    else:\n",
        "      # Structure for SFTTrainer with chatml format\n",
        "      return {\n",
        "          \"messages\": [\n",
        "              {\"role\": \"user\", \"content\": user_content},\n",
        "              {\"role\": \"assistant\", \"content\": techniques_str} # Model learns this string\n",
        "          ]\n",
        "      }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Data preparation helper functions defined (Multi-Label Revised).\")\n",
        "print(\"\\nExample Prompt Structure:\")\n",
        "# Generate an example prompt text (without actual text content)\n",
        "example_prompt_text_only = instruction_prompt_template.format(\n",
        "    labels_string_with_descriptions=labels_string_with_descriptions,\n",
        "    text=\"[Sample text would go here]\"\n",
        ")\n",
        "print(example_prompt_text_only) # Print the first 1000 chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSmHB1rsbgRQ",
        "outputId": "414291db-94f3-4170-d6f0-bd944a9c1152"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preparation helper functions defined (Multi-Label Revised).\n",
            "\n",
            "Example Prompt Structure:\n",
            "Identify ALL applicable manipulation techniques used in the following text.\n",
            "Choose from the list below. If multiple techniques apply, list them separated by commas.\n",
            "\n",
            "Available Techniques:\n",
            "- **appeal_to_fear**: The misuse of fear (often based on stereotypes or prejudices) to support a particular proposal.\n",
            "- **bandwagon**: An attempt to persuade the audience to join and take action because “others are doing the same thing.”\n",
            "- **cherry_picking**: Selective use of data or facts that support a hypothesis while ignoring counterarguments.\n",
            "- **cliche**: Commonly used phrases that mitigate cognitive dissonance and block critical thinking (e.g., “Everything is not so clear-cut”).\n",
            "- **euphoria**: Using an event that causes euphoria or a feeling of happiness, or a positive event to boost morale, often used to mobilize the population.\n",
            "- **fud**: Presenting information in a way that sows uncertainty and doubt, causing fear. A subtype of Appeal to Fear.\n",
            "- **glittering_generalities**: Exploitation of people's positive attitude towards abstract concepts such as “justice,” “freedom,” “democracy,” “patriotism,” “peace,” etc., to provoke strong emotional reactions without specific information.\n",
            "- **loaded_language**: The use of words and phrases with a strong emotional connotation (positive or negative) to influence the audience.\n",
            "- **straw_man**: Distorting the opponent's position by replacing it with a weaker or outwardly similar one and refuting it instead.\n",
            "- **whataboutism**: Discrediting the opponent's position by accusing them of hypocrisy without directly refuting their arguments.\n",
            "\n",
            "Text:\n",
            "[Sample text would go here]\n",
            "\n",
            "Applicable Manipulation Technique(s):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: Data Loading & Processing (REVISED FILTER AGAIN)\n",
        "\n",
        "# --- Load and Process Datasets ---\n",
        "\n",
        "print(\"Loading datasets...\")\n",
        "try:\n",
        "    # Load training data (adjust column names if your CSV differs)\n",
        "    train_dataset_raw = load_dataset(\n",
        "        \"csv\",\n",
        "        data_files={\"train\": train_csv_path},\n",
        "        split=\"train\",\n",
        "        column_names=['id', 'content', 'lang', 'manipulative', 'techniques', 'trigger_words'],\n",
        "        keep_in_memory=False # Keep False for large datasets\n",
        "    )\n",
        "    # Load test data (adjust column names if your CSV differs)\n",
        "    test_dataset_raw = load_dataset(\n",
        "        \"csv\",\n",
        "        data_files={\"test\": test_csv_path},\n",
        "        split=\"test\",\n",
        "        column_names=['id', 'content'], # Only need id and content\n",
        "        keep_in_memory=False\n",
        "    )\n",
        "    print(f\"Loaded {len(train_dataset_raw)} training examples and {len(test_dataset_raw)} test examples.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR loading CSV datasets: {e}\")\n",
        "    print(\"Please ensure CSV files exist at the specified paths and have the expected columns.\")\n",
        "    print(\"Using dummy data for demonstration purposes.\")\n",
        "    # Fallback to dummy data if loading fails\n",
        "    dummy_train_data = {'id': ['id1', 'id2'], 'content': ['Some text 1', 'Some text 2'], 'lang': ['uk', 'uk'], 'manipulative': [True, True], 'techniques': [\"['euphoria']\", \"['loaded_language' 'straw_man']\"], 'trigger_words': ['', '']}\n",
        "    dummy_test_data = {'id': ['test_id1', 'test_id2'], 'content': ['Test text 1', 'Test text 2']}\n",
        "    train_dataset_raw = Dataset.from_dict(dummy_train_data)\n",
        "    test_dataset_raw = Dataset.from_dict(dummy_test_data)\n",
        "\n",
        "\n",
        "# Apply formatting function (returns {'messages': [...] } or {'messages': None})\n",
        "print(\"Formatting training data (Step 1: Structure)...\")\n",
        "original_columns = train_dataset_raw.column_names\n",
        "num_proc = os.cpu_count() // 2 if os.cpu_count() else 1\n",
        "print(f\"Using num_proc={num_proc} for mapping.\")\n",
        "\n",
        "structured_train_dataset = train_dataset_raw.map(\n",
        "    format_data_for_sft, # Function defined in Cell 6\n",
        "    remove_columns=original_columns,\n",
        "    num_proc=num_proc,\n",
        "    batched=False\n",
        ")\n",
        "\n",
        "# Filter out examples where 'messages' is None\n",
        "num_raw_examples = len(train_dataset_raw)\n",
        "filtered_train_dataset = structured_train_dataset.filter(\n",
        "    lambda example: example.get('messages') is not None,\n",
        "    num_proc=num_proc\n",
        ")\n",
        "num_filtered_examples = len(filtered_train_dataset)\n",
        "\n",
        "# --- NEW STEP: Apply chat template to create a 'text' column ---\n",
        "print(\"Applying chat template (Step 2: Format to String)...\")\n",
        "\n",
        "def apply_chat_template_func(example):\n",
        "    \"\"\"Applies the chat template to the 'messages' list.\"\"\"\n",
        "    messages = example.get('messages')\n",
        "    if messages:\n",
        "        try:\n",
        "            # tokenize=False returns the formatted string\n",
        "            # add_generation_prompt=False because SFTTrainer adds it later during training\n",
        "            formatted_text = tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=False # SFTTrainer handles this\n",
        "            )\n",
        "            return {\"text\": formatted_text}\n",
        "        except Exception as e:\n",
        "            print(f\"Error applying chat template: {e}\\nMessages: {messages}\")\n",
        "            return {\"text\": None} # Return None text on error\n",
        "    else:\n",
        "        return {\"text\": None}\n",
        "\n",
        "# Apply the function, keeping only the new 'text' column\n",
        "final_train_dataset = filtered_train_dataset.map(\n",
        "    apply_chat_template_func,\n",
        "    remove_columns=filtered_train_dataset.column_names, # Remove the 'messages' column\n",
        "    num_proc=num_proc,\n",
        "    batched=False\n",
        ")\n",
        "\n",
        "# Filter out any examples where chat template application failed\n",
        "final_train_dataset = final_train_dataset.filter(\n",
        "    lambda example: example.get('text') is not None,\n",
        "    num_proc=num_proc\n",
        ")\n",
        "num_final_examples = len(final_train_dataset)\n",
        "\n",
        "\n",
        "print(\"\\n--- Data Processing Summary ---\")\n",
        "print(f\"Original training examples: {num_raw_examples}\")\n",
        "print(f\"Examples after structure formatting & filtering: {num_filtered_examples}\")\n",
        "print(f\"Examples after applying chat template & final filtering: {num_final_examples}\")\n",
        "skipped_count = num_raw_examples - num_final_examples\n",
        "if skipped_count > 0:\n",
        "    print(f\"NOTE: {skipped_count} examples were skipped in total during processing.\")\n",
        "\n",
        "if num_final_examples > 0:\n",
        "    print(\"\\nExample instance in final 'text' format:\")\n",
        "    print(final_train_dataset[0]['text']) # Display the formatted string\n",
        "else:\n",
        "    print(\"\\nWARNING: No valid training examples remaining after applying chat template! Cannot train.\")\n",
        "    print(\"Check errors during chat template application or earlier steps.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773,
          "referenced_widgets": [
            "e4f283eed8f6487ea8b872157021673d",
            "84ee8cb137c94ccd8a88aa2069d2c37c",
            "5bfe3f8e3a524ae1bc86b4a843d12cee",
            "e4dd0665f009465cafb3db7f75707c97",
            "f11a1b2a0d2b4394baa38b20facd16cd",
            "54ab05ab0572469c8cf4f78db6d3404b",
            "98017d1ac98f45dca6b16703e3345715",
            "1ef0b5ee587e401aa4b53ccdced3eb0e",
            "6d600bb8dcd640c7a7d8e070ac170662",
            "c72ca6f3402e40ff868ea7d33b748b7c",
            "aedcbe29400047e1b7461c853303d7ce",
            "6905d6a1ba774038b6a2a8b0d4e16a1a",
            "b2bda5fc2d4541f69224c6976fdcd85f",
            "95b52542d5d64678956aa3227013079a",
            "e6676d6927a648f5b192fdd0acc53829",
            "adfeb6e583a54abbbed553d8adda0aa4",
            "126ae3927e3a425d8ebe906d9bdcae5d",
            "29f80c18eb144fd3a70220679874f568",
            "0f4cc5e08c9d4ff4bff80ad93541cd77",
            "e53fcd85df794216993a23c7f20624a3",
            "1aef4810ac294872bb213be6b4993f67",
            "15d5bf78b3224ffb8800d3f04d65e899",
            "4b8c68fb4cea40f5a0de14004acfeb90",
            "7cd9a7bc1657408eaa01fa8c815af623",
            "eeecdbda0df448068010a1ba402ae227",
            "691ea887f70646f38b8f47f6b4573c67",
            "0a25d922f665423184c3adbf457bd255",
            "e1c3fc73a3a7493d8f68858a938d331e",
            "456f2ff223784bba998b7b8baab56e62",
            "d23657962aac4a0d96198e353027511b",
            "98dcff9f8eed46d0ac1952029904249f",
            "7235f3499fb94b6bbf2e1a3c3d9c65f7",
            "5d36b0b7b7524f12bfee382e2fc9a605",
            "2e28c54efa8d406c8950a4bef739a038",
            "368cf1ec5b69410caea669cb94040665",
            "19283cefd2c14e7cac80d4ee445549aa",
            "02aa9335e2224bc1b99367396f38e9b2",
            "d3405363ea224ac7b47389ae3ae5b8f5",
            "6a498914516e4ed49c91a8aeb6f4ad4e",
            "ef4d040c546243d6a36593d181cebeb5",
            "d50eb6cadb4e4af5ad0c10e36b65ff16",
            "d5c01aea6ccb4c719bcca3ca49f15fb0",
            "6786c44e026547f9835462fe207be687",
            "f5f4b3b04e6c4066a15a71d5c02e8993"
          ]
        },
        "id": "vd6qYRcybicT",
        "outputId": "0ca58c3d-b8d6-4c0b-85e5-590b8ba326b1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "Loaded 3823 training examples and 5736 test examples.\n",
            "Formatting training data (Step 1: Structure)...\n",
            "Using num_proc=6 for mapping.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=6):   0%|          | 0/3823 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4f283eed8f6487ea8b872157021673d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter (num_proc=6):   0%|          | 0/3823 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6905d6a1ba774038b6a2a8b0d4e16a1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying chat template (Step 2: Format to String)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=6):   0%|          | 0/3823 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b8c68fb4cea40f5a0de14004acfeb90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter (num_proc=6):   0%|          | 0/3823 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e28c54efa8d406c8950a4bef739a038"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Data Processing Summary ---\n",
            "Original training examples: 3823\n",
            "Examples after structure formatting & filtering: 3823\n",
            "Examples after applying chat template & final filtering: 3823\n",
            "\n",
            "Example instance in final 'text' format:\n",
            "<bos><start_of_turn>user\n",
            "Identify ALL applicable manipulation techniques used in the following text.\n",
            "Choose from the list below. If multiple techniques apply, list them separated by commas.\n",
            "\n",
            "Available Techniques:\n",
            "- **appeal_to_fear**: The misuse of fear (often based on stereotypes or prejudices) to support a particular proposal.\n",
            "- **bandwagon**: An attempt to persuade the audience to join and take action because “others are doing the same thing.”\n",
            "- **cherry_picking**: Selective use of data or facts that support a hypothesis while ignoring counterarguments.\n",
            "- **cliche**: Commonly used phrases that mitigate cognitive dissonance and block critical thinking (e.g., “Everything is not so clear-cut”).\n",
            "- **euphoria**: Using an event that causes euphoria or a feeling of happiness, or a positive event to boost morale, often used to mobilize the population.\n",
            "- **fud**: Presenting information in a way that sows uncertainty and doubt, causing fear. A subtype of Appeal to Fear.\n",
            "- **glittering_generalities**: Exploitation of people's positive attitude towards abstract concepts such as “justice,” “freedom,” “democracy,” “patriotism,” “peace,” etc., to provoke strong emotional reactions without specific information.\n",
            "- **loaded_language**: The use of words and phrases with a strong emotional connotation (positive or negative) to influence the audience.\n",
            "- **straw_man**: Distorting the opponent's position by replacing it with a weaker or outwardly similar one and refuting it instead.\n",
            "- **whataboutism**: Discrediting the opponent's position by accusing them of hypocrisy without directly refuting their arguments.\n",
            "\n",
            "Text:\n",
            "content\n",
            "\n",
            "Applicable Manipulation Technique(s):<end_of_turn>\n",
            "<start_of_turn>model\n",
            "techniques<end_of_turn>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configure Training Arguments ---\n",
        "print(\"Configuring training arguments...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_train_epochs=num_epochs,\n",
        "    # max_steps=max_steps, # Uncomment to limit steps instead of epochs\n",
        "    learning_rate=learning_rate,\n",
        "    bf16=compute_dtype == torch.bfloat16, # Enable bf16 if supported\n",
        "    fp16=compute_dtype == torch.float16, # Enable fp16 if bf16 not supported but fp16 is\n",
        "    logging_steps=logging_steps,\n",
        "    optim=\"adamw_torch\", # Standard optimizer\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\", # Or \"cosine\"\n",
        "    seed=seed,\n",
        "    output_dir=output_dir,\n",
        "    save_strategy=\"epoch\", # Save at the end of each epoch (or \"steps\")\n",
        "    # save_steps=save_steps, # Define frequency if save_strategy=\"steps\"\n",
        "    save_total_limit=3,     # Keep only the last checkpoint\n",
        "    report_to=\"none\",     # Disable wandb/tensorboard reporting unless configured\n",
        "    dataloader_num_workers = 2, # Adjust based on your system for data loading speed\n",
        ")\n",
        "\n",
        "print(f\"Training arguments configured. Output directory: {output_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhtqYhEebmIN",
        "outputId": "cf4c16ed-ed86-428f-83fb-6161048c6857"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring training arguments...\n",
            "Training arguments configured. Output directory: manipulation_classifier_finetune\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 9: Initialize & Run SFT Trainer (Use 'text' Field)\n",
        "\n",
        "# --- Initialize and Run Trainer ---\n",
        "\n",
        "trainer = None\n",
        "final_adapter_path = None\n",
        "\n",
        "# Use num_final_examples from Cell 7\n",
        "if num_final_examples > 0:\n",
        "    print(\"Initializing SFTTrainer...\")\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=final_train_dataset, # Use the dataset with the 'text' column\n",
        "        args=training_args,\n",
        "        max_seq_length=max_seq_length,\n",
        "        dataset_text_field=\"text\", # <<< TELL TRAINER TO USE THE 'text' COLUMN\n",
        "        # formatting_func=... # <<< REMOVE formatting_func\n",
        "        # packing=True, # Consider packing=True if sequences are short relative to max_seq_length\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    try:\n",
        "        # train_result = trainer.train()\n",
        "        train_result = trainer.train(resume_from_checkpoint=\"/content/manipulation_classifier_finetune/checkpoint-240\")\n",
        "        print(\"--- Training Finished ---\")\n",
        "\n",
        "        # --- Save Final Adapters ---\n",
        "        print(\"Saving final LoRA adapters...\")\n",
        "        final_adapter_path = os.path.join(output_dir, \"final_adapters\")\n",
        "        trainer.model.save_pretrained(final_adapter_path)\n",
        "        tokenizer.save_pretrained(final_adapter_path)\n",
        "        print(f\"Final LoRA adapters saved to: {final_adapter_path}\")\n",
        "\n",
        "        # Optional: Log metrics\n",
        "        metrics = train_result.metrics\n",
        "        print(\"Training Metrics:\", metrics)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during training: {e}\")\n",
        "        # final_adapter_path remains None\n",
        "\n",
        "else:\n",
        "    print(\"Skipping training because no valid formatted training data is available.\")\n",
        "    # final_adapter_path remains None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "OiK2GL1Nl0fm",
        "outputId": "766d45f1-9f92-499f-8b6d-2ca60ac69874"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing SFTTrainer...\n",
            "\n",
            "--- Starting Training ---\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 3,823 | Num Epochs = 14 | Total steps = 420\n",
            "O^O/ \\_/ \\    Batch size per device = 64 | Gradient accumulation steps = 2\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (64 x 2 x 1) = 128\n",
            " \"-____-\"     Trainable parameters = 6,522,880/1,006,408,832 (0.65% trained)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='367' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [367/420 36:06 < 15:18, 0.06 it/s, Epoch 12.20/14]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.283700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.258200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>1.245300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.217900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>1.274200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.206100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>1.234200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.234700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>1.206600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.206300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.185400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.270800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 10: Inference Setup - Load Trained Model from Specific Checkpoint\n",
        "\n",
        "# --- Prepare Model for Inference ---\n",
        "\n",
        "model_for_inference = None\n",
        "tokenizer_for_inference = tokenizer # Use the tokenizer loaded earlier (should match base model)\n",
        "\n",
        "# --- Specify the checkpoint path to load ---\n",
        "specific_checkpoint_path = \"/content/manipulation_classifier_finetune/checkpoint-420\"\n",
        "print(f\"Attempting to load adapters from checkpoint: {specific_checkpoint_path}\")\n",
        "\n",
        "# Check if the specified checkpoint path exists\n",
        "if specific_checkpoint_path and os.path.exists(specific_checkpoint_path):\n",
        "    print(f\"Checkpoint directory found. Loading base model and merging adapters...\")\n",
        "\n",
        "    # Determine dtype for inference (usually same as training or float16)\n",
        "    inference_dtype = compute_dtype if compute_dtype != torch.float32 else torch.float16\n",
        "    print(f\"Using inference dtype: {inference_dtype}\")\n",
        "\n",
        "    # Reload the base model in the desired inference precision\n",
        "    # Important: Load base model WITHOUT quantization for merging adapters\n",
        "    print(f\"Reloading base model ({model_name}) in {inference_dtype}...\")\n",
        "    try:\n",
        "        base_model, tokenizer_for_inference = FastModel.from_pretrained(\n",
        "            model_name=model_name,\n",
        "            max_seq_length=max_seq_length,\n",
        "            dtype=inference_dtype,\n",
        "            load_in_4bit=False, # Must load in float16/bfloat16 to merge\n",
        "            load_in_8bit=False,\n",
        "            # token = \"hf_...\", # Add token if needed\n",
        "        )\n",
        "        print(\"Base model reloaded.\")\n",
        "\n",
        "        # Load the LoRA adapters onto the base model FROM THE CHECKPOINT PATH\n",
        "        print(f\"Applying trained LoRA adapters from {specific_checkpoint_path}...\")\n",
        "        # Make sure PeftModel loads correctly from the checkpoint subdirectory\n",
        "        model_for_inference = PeftModel.from_pretrained(base_model, specific_checkpoint_path)\n",
        "        print(\"Adapters loaded.\")\n",
        "\n",
        "        # Merge the adapters into the base model and unload PeftModel\n",
        "        print(\"Merging adapters...\")\n",
        "        model_for_inference = model_for_inference.merge_and_unload()\n",
        "        print(\"Adapters merged. Model ready for inference.\")\n",
        "\n",
        "        # Set to evaluation mode and move to GPU if available\n",
        "        model_for_inference.eval()\n",
        "        if torch.cuda.is_available():\n",
        "            print(\"Moving model to GPU...\")\n",
        "            model_for_inference.to(\"cuda\")\n",
        "            print(\"Model on GPU.\")\n",
        "        else:\n",
        "            print(\"Warning: No GPU available for inference.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR loading base model or applying/merging adapters: {e}\")\n",
        "        model_for_inference = None # Ensure model is None if loading fails\n",
        "\n",
        "else:\n",
        "    print(f\"ERROR: Specified checkpoint path not found: {specific_checkpoint_path}\")\n",
        "    print(\"Skipping inference setup.\")\n",
        "    model_for_inference = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMfn7foxbmxo",
        "outputId": "099aaade-ecee-42fa-aea3-ba18171ae713"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load adapters from checkpoint: /content/manipulation_classifier_finetune/checkpoint-420\n",
            "Checkpoint directory found. Loading base model and merging adapters...\n",
            "Using inference dtype: torch.bfloat16\n",
            "Reloading base model (unsloth/gemma-3-1b-it-unsloth-bnb-4bit) in torch.bfloat16...\n",
            "==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.50.0.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n",
            "Base model reloaded.\n",
            "Applying trained LoRA adapters from /content/manipulation_classifier_finetune/checkpoint-420...\n",
            "Adapters loaded.\n",
            "Merging adapters...\n",
            "Adapters merged. Model ready for inference.\n",
            "Moving model to GPU...\n",
            "Model on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 11: Generate and Save RAW Predictions\n",
        "\n",
        "# Check if the inference model is ready\n",
        "if model_for_inference is not None and test_dataset_raw is not None:\n",
        "    print(\"\\n--- Generating Raw Predictions ---\")\n",
        "    raw_results = [] # Store dictionaries {'id': ..., 'raw_prediction': ...}\n",
        "    total_test_examples = len(test_dataset_raw)\n",
        "    print(f\"Processing {total_test_examples} test examples...\")\n",
        "\n",
        "    # Set generation parameters\n",
        "    generation_max_new_tokens = 100 # Max length for expected label list\n",
        "    generation_temperature = 0.1\n",
        "    generation_do_sample = True # Use sampling with low temp\n",
        "    generation_top_p = 0.9\n",
        "\n",
        "    for i, example in enumerate(test_dataset_raw):\n",
        "        test_id = example.get('id')\n",
        "        test_content = example.get('content')\n",
        "\n",
        "        if not test_id or not test_content:\n",
        "            print(f\"Warning: Skipping test example at index {i} due to missing id or content.\")\n",
        "            continue\n",
        "\n",
        "        # 1. Format input using the prompt structure\n",
        "        user_content = instruction_prompt_template.format(\n",
        "            labels_string_with_descriptions=labels_string_with_descriptions, # Use detailed prompt\n",
        "            text=test_content\n",
        "        )\n",
        "        messages = [{\"role\": \"user\", \"content\": user_content}]\n",
        "        input_text = tokenizer_for_inference.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        # 2. Tokenize input\n",
        "        inputs = tokenizer_for_inference(\n",
        "            input_text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=max_seq_length - generation_max_new_tokens # Reserve space\n",
        "        )\n",
        "\n",
        "        # 3. Move inputs to GPU if available\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = inputs.to(\"cuda\")\n",
        "\n",
        "        # 4. Generate prediction\n",
        "        predicted_text_raw = \"\" # Initialize\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                outputs = model_for_inference.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=generation_max_new_tokens,\n",
        "                    temperature=generation_temperature,\n",
        "                    do_sample=generation_do_sample,\n",
        "                    top_p=generation_top_p,\n",
        "                    pad_token_id=tokenizer_for_inference.eos_token_id,\n",
        "                )\n",
        "\n",
        "            # 5. Decode raw output string\n",
        "            prediction_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
        "            predicted_text_raw = tokenizer_for_inference.decode(prediction_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during generation for ID {test_id}: {e}\")\n",
        "            predicted_text_raw = \"[GENERATION_ERROR]\" # Placeholder on error\n",
        "\n",
        "        # 6. Store ID and Raw Prediction\n",
        "        raw_results.append({'id': test_id, 'raw_prediction': predicted_text_raw})\n",
        "\n",
        "        # Print progress\n",
        "        if (i + 1) % 50 == 0 or (i + 1) == total_test_examples:\n",
        "             print(f\"Processed {i+1}/{total_test_examples}... Raw Output: '{predicted_text_raw[:60]}...'\")\n",
        "\n",
        "    # --- Save Raw Results to CSV ---\n",
        "    raw_predictions_path = \"raw_predictions.csv\"\n",
        "    print(f\"\\nSaving raw predictions to: {raw_predictions_path}\")\n",
        "    if raw_results:\n",
        "        raw_df = pd.DataFrame(raw_results)\n",
        "        raw_df.to_csv(raw_predictions_path, index=False)\n",
        "        print(\"Raw predictions saved.\")\n",
        "        print(\"\\nRaw predictions sample:\")\n",
        "        print(raw_df.head())\n",
        "    else:\n",
        "        print(\"No raw results generated.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping raw prediction generation: Inference model not loaded or test data missing.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff5lOi_gdsK4",
        "outputId": "b4b5e23d-f47b-4660-da90-d4a65b699f60"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating Raw Predictions ---\n",
            "Processing 5736 test examples...\n",
            "Processed 50/5736... Raw Output: '['cherry_picking' 'loaded_language']...'\n",
            "Processed 100/5736... Raw Output: '['loaded_language' 'glittering_generalities']...'\n",
            "Processed 150/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 200/5736... Raw Output: '['cliche' 'fud' 'glittering_generalities']...'\n",
            "Processed 250/5736... Raw Output: '['cliche' 'euphoria' 'fud' 'glittering_generalities' 'loaded...'\n",
            "Processed 300/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 350/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 400/5736... Raw Output: '['euphoria' 'glittering_generalities']...'\n",
            "Processed 450/5736... Raw Output: '['straw_man']...'\n",
            "Processed 500/5736... Raw Output: '['loaded_language' 'euphoria']...'\n",
            "Processed 550/5736... Raw Output: '['cherry_picking' 'loaded_language']...'\n",
            "Processed 600/5736... Raw Output: '['euphoria' 'loaded_language' 'cherry_picking']...'\n",
            "Processed 650/5736... Raw Output: '['loaded_language' 'euphoria']...'\n",
            "Processed 700/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 750/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 800/5736... Raw Output: '['fud' 'glittering_generalities']...'\n",
            "Processed 850/5736... Raw Output: '['loaded_language' 'fud']...'\n",
            "Processed 900/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 950/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 1000/5736... Raw Output: '['euphoria' 'loaded_language' 'glittering_generalities']...'\n",
            "Processed 1050/5736... Raw Output: '['appeal_to_fear', 'loaded_language']...'\n",
            "Processed 1100/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 1150/5736... Raw Output: '['loaded_language' 'fud']...'\n",
            "Processed 1200/5736... Raw Output: '['loaded_language' 'euphoria']...'\n",
            "Processed 1250/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 1300/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 1350/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 1400/5736... Raw Output: '['loaded_language' 'euphoria']...'\n",
            "Processed 1450/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 1500/5736... Raw Output: '['euphoria']...'\n",
            "Processed 1550/5736... Raw Output: '['loaded_language' 'euphoria']...'\n",
            "Processed 1600/5736... Raw Output: '['euphoria' 'cherry_picking']...'\n",
            "Processed 1650/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 1700/5736... Raw Output: '['loaded_language' 'euphoria']...'\n",
            "Processed 1750/5736... Raw Output: '['loaded_language' 'glittering_generalities']...'\n",
            "Processed 1800/5736... Raw Output: '['cliche' 'glittering_generalities' 'loaded_language']...'\n",
            "Processed 1850/5736... Raw Output: '['loaded_language' 'cliche']...'\n",
            "Processed 1900/5736... Raw Output: '['loaded_language' 'fud']...'\n",
            "Processed 1950/5736... Raw Output: '['cherry_picking' 'euphoria']...'\n",
            "Processed 2000/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 2050/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 2100/5736... Raw Output: '['loaded_language' 'fud']...'\n",
            "Processed 2150/5736... Raw Output: '['loaded_language' 'fud']...'\n",
            "Processed 2200/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 2250/5736... Raw Output: '['loaded_language' 'euphoria']...'\n",
            "Processed 2300/5736... Raw Output: '['loaded_language' 'fud' 'glittering_generalities']...'\n",
            "Processed 2350/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 2400/5736... Raw Output: '['cliche' 'fud' 'glittering_generalities' 'loaded_language' ...'\n",
            "Processed 2450/5736... Raw Output: '['cliche' 'euphoria' 'bandwagon']...'\n",
            "Processed 2500/5736... Raw Output: '['cliche' 'fud' 'glittering_generalities' 'loaded_language' ...'\n",
            "Processed 2550/5736... Raw Output: '['loaded_language' 'fud']...'\n",
            "Processed 2600/5736... Raw Output: '['euphoria' 'loaded_language' 'fud']...'\n",
            "Processed 2650/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 2700/5736... Raw Output: '['loaded_language' 'fud']...'\n",
            "Processed 2750/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 2800/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 2850/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 2900/5736... Raw Output: '['loaded_language' 'euphoria']...'\n",
            "Processed 2950/5736... Raw Output: '['appeal_to_fear' 'loaded_language']...'\n",
            "Processed 3000/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 3050/5736... Raw Output: '['loaded_language' 'fud']...'\n",
            "Processed 3100/5736... Raw Output: '['loaded_language' 'fud']...'\n",
            "Processed 3150/5736... Raw Output: '['euphoria' 'loaded_language' 'cherry_picking']...'\n",
            "Processed 3200/5736... Raw Output: '['euphoria']...'\n",
            "Processed 3250/5736... Raw Output: '['loaded_language' 'euphoria']...'\n",
            "Processed 3300/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 3350/5736... Raw Output: '['straw_man' '...'\n",
            "Processed 3400/5736... Raw Output: '['loaded_language' 'fud']...'\n",
            "Processed 3450/5736... Raw Output: '['loaded_language' 'euphoria']...'\n",
            "Processed 3500/5736... Raw Output: '['euphoria' 'loaded_language' 'glittering_generalities']...'\n",
            "Processed 3550/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 3600/5736... Raw Output: '['fud' 'glittering_generalities']...'\n",
            "Processed 3650/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 3700/5736... Raw Output: '['loaded_language' 'glittering_generalities']...'\n",
            "Processed 3750/5736... Raw Output: '['loaded_language' 'glittering_generalities']...'\n",
            "Processed 3800/5736... Raw Output: '['loaded_language' 'euphoria']...'\n",
            "Processed 3850/5736... Raw Output: '['cherry_picking' 'euphoria']...'\n",
            "Processed 3900/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 3950/5736... Raw Output: '['loaded_language' 'euphoria']...'\n",
            "Processed 4000/5736... Raw Output: '['loaded_language' 'fud']...'\n",
            "Processed 4050/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 4100/5736... Raw Output: '['euphoria' 'straw_man']...'\n",
            "Processed 4150/5736... Raw Output: '['loaded_language' 'euphoria']...'\n",
            "Processed 4200/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 4250/5736... Raw Output: '['loaded_language' 'fud']...'\n",
            "Processed 4300/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 4350/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 4400/5736... Raw Output: '['euphoria' 'loaded_language' 'cherry_picking']...'\n",
            "Processed 4450/5736... Raw Output: '['loaded_language' 'fud']...'\n",
            "Processed 4500/5736... Raw Output: '['euphoria' 'loaded_language']...'\n",
            "Processed 4550/5736... Raw Output: '['loaded_language' 'fud']...'\n",
            "Processed 4600/5736... Raw Output: '...'\n",
            "Processed 4650/5736... Raw Output: '['loaded_language' 'cliche']...'\n",
            "Processed 4700/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 4750/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 4800/5736... Raw Output: '['cliche' 'glittering_generalities' 'loaded_language']...'\n",
            "Processed 4850/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 4900/5736... Raw Output: '...'\n",
            "Processed 4950/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 5000/5736... Raw Output: '['loaded_language' 'fud']...'\n",
            "Processed 5050/5736... Raw Output: '['euphoria']...'\n",
            "Processed 5100/5736... Raw Output: '['cliche' 'loaded_language']...'\n",
            "Processed 5150/5736... Raw Output: '['loaded_language' 'fud']...'\n",
            "Processed 5200/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 5250/5736... Raw Output: '['loaded_language' 'euphoria']...'\n",
            "Processed 5300/5736... Raw Output: '['euphoria' 'loaded_language' 'cherry_picking']...'\n",
            "Processed 5350/5736... Raw Output: '['cherry_picking' 'euphoria']...'\n",
            "Processed 5400/5736... Raw Output: '['loaded_language' 'euphoria']...'\n",
            "Processed 5450/5736... Raw Output: '['cherry_picking' 'loaded_language']...'\n",
            "Processed 5500/5736... Raw Output: '['loaded_language' 'whataboutism']...'\n",
            "Processed 5550/5736... Raw Output: '['cliche' 'euphoria' 'bandwagon']...'\n",
            "Processed 5600/5736... Raw Output: '['cliche' 'glittering_generalities' 'loaded_language']...'\n",
            "Processed 5650/5736... Raw Output: '['cherry_picking' 'loaded_language']...'\n",
            "Processed 5700/5736... Raw Output: '['loaded_language']...'\n",
            "Processed 5736/5736... Raw Output: '['loaded_language' 'fud']...'\n",
            "\n",
            "Saving raw predictions to: raw_predictions.csv\n",
            "Raw predictions saved.\n",
            "\n",
            "Raw predictions sample:\n",
            "                                     id                  raw_prediction\n",
            "0                                    id              ['appeal_to_fear']\n",
            "1  521cd2e8-dd9f-42c4-98ba-c0c8890ff1ba    ['loaded_language' 'cliche']\n",
            "2  9b2a61e4-d14e-4ff7-b304-e73d720319bf             ['loaded_language']\n",
            "3  f0f1c236-80a8-4d25-b30c-a420a39be632       ['loaded_language' 'fud']\n",
            "4  31ea05ba-2c2b-4b84-aba7-f3cf6841b204  ['loaded_language' 'euphoria']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 12: Stage 1 - Parse Strict Format & Separate Exceptions (Includes Empty as Good)\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np # Import numpy for NaN checking\n",
        "\n",
        "# --- Configuration (Ensure SUBMISSION_LABELS is available from Cell 3) ---\n",
        "# SUBMISSION_LABELS = sorted([...]) # Copy from Cell 3 if needed\n",
        "\n",
        "raw_predictions_path = \"raw_predictions.csv\"\n",
        "good_rows_output_path = \"submission_part1_good_format.csv\" # Output for rows with definite parsed labels (0 or more)\n",
        "exceptions_output_path = \"parsing_exceptions.csv\" # Output for rows with parsing errors or unrecognized text parts\n",
        "\n",
        "# --- Regex Pattern for ['label' 'label'] format (from Cell 6) ---\n",
        "technique_pattern = re.compile(r\"'(.*?)'\")\n",
        "\n",
        "# Check if raw predictions file exists\n",
        "if os.path.exists(raw_predictions_path):\n",
        "    print(f\"Loading raw predictions from {raw_predictions_path}...\")\n",
        "    try:\n",
        "        raw_df = pd.read_csv(raw_predictions_path).replace({np.nan: None}) # Load and replace NaN\n",
        "        print(f\"Loaded {len(raw_df)} raw predictions.\")\n",
        "\n",
        "        # --- Prepare for Validation ---\n",
        "        lowercase_to_canonical_label = {label.lower(): label for label in SUBMISSION_LABELS}\n",
        "        valid_lowercase_labels = set(lowercase_to_canonical_label.keys())\n",
        "\n",
        "        # --- Initialize Lists for Results ---\n",
        "        good_submission_rows = []\n",
        "        exception_rows = []\n",
        "        print(\"Parsing predictions - Stage 1: Creating rows for strictly formatted AND empty/no-label predictions...\")\n",
        "\n",
        "        for index, row in raw_df.iterrows():\n",
        "            test_id = row['id']\n",
        "            raw_prediction = row['raw_prediction']\n",
        "\n",
        "            parsed_labels_for_row = set()\n",
        "            potential_labels_to_validate = []\n",
        "            invalid_parts_found = 0\n",
        "            is_exception = False # Flag for definite exceptions\n",
        "\n",
        "            # --- Basic Checks ---\n",
        "            # Treat None, non-strings, or explicit errors as needing review (exceptions)\n",
        "            # Treat empty/whitespace-only strings as \"good\" (resulting in all zeros)\n",
        "            if raw_prediction is None or not isinstance(raw_prediction, str):\n",
        "                # Keep parsed_labels_for_row empty, proceed to create all-zero row\n",
        "                 pass # No invalid parts found yet\n",
        "            elif not raw_prediction.strip():\n",
        "                 # Empty string is considered \"good\" -> all zeros\n",
        "                 pass # No invalid parts found yet\n",
        "            elif \"[GENERATION_ERROR]\" in raw_prediction:\n",
        "                 is_exception = True # Definitely an exception\n",
        "            else:\n",
        "                # --- Attempt Strict Parsing ---\n",
        "                cleaned_prediction = raw_prediction.strip()\n",
        "\n",
        "                # Check for ['label' 'label'] format FIRST\n",
        "                if cleaned_prediction.startswith(\"['\") and cleaned_prediction.endswith(\"']\"):\n",
        "                    potential_labels_to_validate = technique_pattern.findall(cleaned_prediction)\n",
        "                    if not potential_labels_to_validate and len(cleaned_prediction) > 2:\n",
        "                         invalid_parts_found += 1 # Malformed bracketed string\n",
        "\n",
        "                # ELSE, assume comma-separated format (or single label)\n",
        "                else:\n",
        "                    potential_labels_to_validate = cleaned_prediction.split(',')\n",
        "\n",
        "                # --- Validate the extracted potential labels ---\n",
        "                # Only validate if no invalid parts flagged yet from format check\n",
        "                if invalid_parts_found == 0 and potential_labels_to_validate:\n",
        "                    for part in potential_labels_to_validate:\n",
        "                        cleaned_part = part.strip()\n",
        "                        if not cleaned_part: # Ignore empty parts between commas etc.\n",
        "                            continue\n",
        "\n",
        "                        cleaned_part_lower = cleaned_part.lower()\n",
        "\n",
        "                        if cleaned_part_lower in valid_lowercase_labels:\n",
        "                            # Part is valid\n",
        "                            canonical_label = lowercase_to_canonical_label[cleaned_part_lower]\n",
        "                            parsed_labels_for_row.add(canonical_label)\n",
        "                        else:\n",
        "                            # Part is NOT a valid label -> THIS IS AN EXCEPTION\n",
        "                            invalid_parts_found += 1\n",
        "                            is_exception = True # Mark as definite exception\n",
        "                            break # Stop processing this row\n",
        "\n",
        "            # --- Assign to Correct List ---\n",
        "            if is_exception or invalid_parts_found > 0:\n",
        "                # Add to exceptions if explicit error, malformed, or contained invalid text parts\n",
        "                exception_rows.append({'id': test_id, 'raw_prediction': raw_prediction})\n",
        "            else:\n",
        "                # Otherwise, it's considered \"good\" - either parsed correctly OR resulted in zero labels cleanly\n",
        "                # Create the final submission row dictionary (will have all zeros if parsed_labels_for_row is empty)\n",
        "                submission_row = {'id': test_id}\n",
        "                for label in SUBMISSION_LABELS:\n",
        "                    submission_row[label] = 1 if label in parsed_labels_for_row else 0\n",
        "                good_submission_rows.append(submission_row)\n",
        "\n",
        "            # Optional: Print progress\n",
        "            if (index + 1) % 500 == 0 or (index + 1) == len(raw_df):\n",
        "                status = \"EXCEPTION\" if (is_exception or invalid_parts_found > 0) else \"GOOD\"\n",
        "                found_labels_str = str(parsed_labels_for_row) if status == \"GOOD\" else \"N/A\"\n",
        "                print(f\"Processed {index+1}/{len(raw_df)}... ID: {test_id} -> Status: {status}, Found: {found_labels_str}\")\n",
        "\n",
        "\n",
        "        # --- Save Good Rows ---\n",
        "        if good_submission_rows:\n",
        "            print(f\"\\nGenerated {len(good_submission_rows)} rows for submission (including all-zero rows).\")\n",
        "            good_df = pd.DataFrame(good_submission_rows)\n",
        "            final_columns = ['id'] + SUBMISSION_LABELS\n",
        "            good_df = good_df.reindex(columns=final_columns, fill_value=0)\n",
        "            print(f\"Saving 'good' rows (including empty) to: {good_rows_output_path}\")\n",
        "            good_df.to_csv(good_rows_output_path, index=False)\n",
        "            print(\"'Good' rows saved.\")\n",
        "        else:\n",
        "            print(\"\\nNo 'good' rows generated for submission.\")\n",
        "\n",
        "        # --- Save Exception Rows ---\n",
        "        if exception_rows:\n",
        "            print(f\"\\nFound {len(exception_rows)} exceptions (malformed or contained unrecognized text).\")\n",
        "            exceptions_df = pd.DataFrame(exception_rows)\n",
        "            print(f\"Saving exceptions to: {exceptions_output_path}\")\n",
        "            exceptions_df.to_csv(exceptions_output_path, index=False)\n",
        "            print(\"Exceptions saved for review.\")\n",
        "        else:\n",
        "            print(\"\\nNo exceptions found.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during Stage 1 processing: {e}\")\n",
        "\n",
        "else:\n",
        "    print(f\"ERROR: Raw predictions file not found at {raw_predictions_path}. Cannot process Stage 1.\")\n",
        "\n",
        "print(\"\\n--- Stage 1 Processing Finished ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrHHi9RDdOgT",
        "outputId": "99f3b350-e81f-41e4-d440-a89480c43819"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading raw predictions from raw_predictions.csv...\n",
            "Loaded 5736 raw predictions.\n",
            "Parsing predictions - Stage 1: Creating rows for strictly formatted AND empty/no-label predictions...\n",
            "Processed 500/5736... ID: c172c4c3-6753-4a7f-818b-505c156eb3f1 -> Status: GOOD, Found: {'loaded_language', 'euphoria'}\n",
            "Processed 1000/5736... ID: 0f2ed9ad-ed12-4e49-bfd3-2c2342780344 -> Status: GOOD, Found: {'loaded_language', 'euphoria', 'glittering_generalities'}\n",
            "Processed 1500/5736... ID: 3f10588b-d226-428c-ba0e-272b1c2b6bd0 -> Status: GOOD, Found: {'euphoria'}\n",
            "Processed 2000/5736... ID: cb1ec277-8304-433f-85a4-772694f93c1e -> Status: GOOD, Found: {'loaded_language'}\n",
            "Processed 2500/5736... ID: 0ee21de4-c1b3-4329-9640-1eb533e16558 -> Status: GOOD, Found: {'glittering_generalities', 'loaded_language', 'fud', 'cliche', 'straw_man'}\n",
            "Processed 3000/5736... ID: 4152e588-7bc5-4221-902d-d18fda24028f -> Status: GOOD, Found: {'loaded_language'}\n",
            "Processed 3500/5736... ID: e6e58fbc-6564-4690-be62-370b1a2dbb15 -> Status: GOOD, Found: {'loaded_language', 'euphoria', 'glittering_generalities'}\n",
            "Processed 4000/5736... ID: 282a686a-1b5a-42b3-8e9f-5bb4a56815c7 -> Status: GOOD, Found: {'loaded_language', 'fud'}\n",
            "Processed 4500/5736... ID: 982a1499-ef1c-464c-9072-c61c3f31ac74 -> Status: GOOD, Found: {'loaded_language', 'euphoria'}\n",
            "Processed 5000/5736... ID: 16bb3707-48f9-4417-98b0-52601aa63868 -> Status: GOOD, Found: {'loaded_language', 'fud'}\n",
            "Processed 5500/5736... ID: 80bdecd4-a6f3-4361-88ab-fd7360038de1 -> Status: GOOD, Found: {'loaded_language', 'whataboutism'}\n",
            "Processed 5736/5736... ID: 5e68b0a8-e87c-45a3-a53a-7bde2e73471a -> Status: GOOD, Found: {'loaded_language', 'fud'}\n",
            "\n",
            "Generated 5638 rows for submission (including all-zero rows).\n",
            "Saving 'good' rows (including empty) to: submission_part1_good_format.csv\n",
            "'Good' rows saved.\n",
            "\n",
            "Found 98 exceptions (malformed or contained unrecognized text).\n",
            "Saving exceptions to: parsing_exceptions.csv\n",
            "Exceptions saved for review.\n",
            "\n",
            "--- Stage 1 Processing Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 13: Stage 2 - Regex Parsing for Exceptions & Final Submission (Get Content from test_dataset_raw)\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- Configuration (Ensure SUBMISSION_LABELS is available) ---\n",
        "# SUBMISSION_LABELS = sorted([...]) # Copy from Cell 3 if needed\n",
        "\n",
        "# --- Input / Output Files ---\n",
        "exceptions_input_path = \"parsing_exceptions_fixed.csv\" # OR \"parsing_exceptions_retry.csv\", etc.\n",
        "stage1_good_rows_path = \"submission_part1_good_format.csv\" # OR \"submission_part1_retry.csv\", etc.\n",
        "stage2_exceptions_output_path = \"parsing_exceptions_stage2_with_content.csv\"\n",
        "final_submission_path = \"submission.csv\"\n",
        "\n",
        "# --- Load Original Test Content from test_dataset_raw ---\n",
        "test_content_map = {}\n",
        "# Check if test_dataset_raw (from Cell 7) exists and has data\n",
        "if 'test_dataset_raw' in locals() and test_dataset_raw is not None and len(test_dataset_raw) > 0:\n",
        "    print(\"Creating content map from 'test_dataset_raw' object...\")\n",
        "    try:\n",
        "        # Iterate through the dataset object once to build the map\n",
        "        for example in test_dataset_raw:\n",
        "            test_content_map[example['id']] = example['content']\n",
        "        print(f\"Created content map for {len(test_content_map)} test IDs.\")\n",
        "        if len(test_content_map) != len(test_dataset_raw):\n",
        "             print(f\"Warning: Number of items in map ({len(test_content_map)}) differs from test_dataset_raw size ({len(test_dataset_raw)}). Possible duplicate IDs?\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not create content map from 'test_dataset_raw': {e}\")\n",
        "        print(\"Final exceptions file will not contain original content.\")\n",
        "else:\n",
        "    print(\"Warning: 'test_dataset_raw' object not found or empty. Cannot add content to exceptions.\")\n",
        "    print(\"Consider re-running Cell 7 or using the previous version of Cell 13 that loads test.csv.\")\n",
        "\n",
        "\n",
        "# --- Check if Stage 1 Exceptions File Exists ---\n",
        "if os.path.exists(exceptions_input_path):\n",
        "    print(f\"\\nLoading Stage 1 exceptions from {exceptions_input_path}...\")\n",
        "    try:\n",
        "        exceptions_df_s1 = pd.read_csv(exceptions_input_path).replace({np.nan: None}) # Load/clean\n",
        "        print(f\"Loaded {len(exceptions_df_s1)} exceptions for final processing.\")\n",
        "\n",
        "        # --- Prepare Regex and Validation Info ---\n",
        "        if 'SUBMISSION_LABELS' not in locals():\n",
        "             print(\"ERROR: SUBMISSION_LABELS not defined. Please run Cell 3 or define it.\")\n",
        "             raise NameError(\"SUBMISSION_LABELS not defined\")\n",
        "\n",
        "        lowercase_to_canonical_label = {label.lower(): label for label in SUBMISSION_LABELS}\n",
        "        pattern_string = r'\\b(' + '|'.join(map(re.escape, lowercase_to_canonical_label.keys())) + r')\\b'\n",
        "        label_pattern = re.compile(pattern_string, re.IGNORECASE)\n",
        "        print(f\"Using regex pattern: {label_pattern.pattern}\")\n",
        "\n",
        "        # --- Initialize Lists for Stage 2 Results ---\n",
        "        stage2_good_rows = []\n",
        "        stage2_exception_rows = []\n",
        "        print(\"Parsing exceptions - Stage 2: Applying lenient regex...\")\n",
        "\n",
        "        for index, row in exceptions_df_s1.iterrows():\n",
        "            test_id = row['id']\n",
        "            raw_prediction = row['raw_prediction']\n",
        "            parsed_labels_for_row = set()\n",
        "            found_labels_lower = []\n",
        "\n",
        "            # --- Apply Regex ---\n",
        "            if raw_prediction and isinstance(raw_prediction, str):\n",
        "                try:\n",
        "                    found_labels_lower = label_pattern.findall(raw_prediction.lower())\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Regex error on ID {test_id}, Raw: '{raw_prediction}'. Error: {e}\")\n",
        "                    found_labels_lower = []\n",
        "\n",
        "            # --- Process Matches ---\n",
        "            if found_labels_lower: # If regex found at least one label\n",
        "                for label_lower in found_labels_lower:\n",
        "                    canonical_label = lowercase_to_canonical_label.get(label_lower)\n",
        "                    if canonical_label:\n",
        "                        parsed_labels_for_row.add(canonical_label)\n",
        "\n",
        "                submission_row = {'id': test_id}\n",
        "                for label in SUBMISSION_LABELS:\n",
        "                    submission_row[label] = 1 if label in parsed_labels_for_row else 0\n",
        "                stage2_good_rows.append(submission_row)\n",
        "            else:\n",
        "                # Regex found NO labels, this is a final exception\n",
        "                # Retrieve original content using the map created from test_dataset_raw\n",
        "                original_content = test_content_map.get(test_id, \"[CONTENT NOT FOUND IN test_dataset_raw]\") # Get content\n",
        "                stage2_exception_rows.append({\n",
        "                    'id': test_id,\n",
        "                    'raw_prediction': raw_prediction,\n",
        "                    'content': original_content # <<< ADDED CONTENT FROM MAP\n",
        "                })\n",
        "\n",
        "            # Optional: Print progress\n",
        "            if (index + 1) % 500 == 0 or (index + 1) == len(exceptions_df_s1):\n",
        "                 status = \"RECOVERED\" if found_labels_lower else \"EXCEPTION_FINAL\"\n",
        "                 found_labels_str = str(parsed_labels_for_row) if found_labels_lower else \"None\"\n",
        "                 print(f\"Processed S2 {index+1}/{len(exceptions_df_s1)}... ID: {test_id} -> Status: {status}, Found: {found_labels_str}\")\n",
        "\n",
        "\n",
        "        # --- Save Final (Stage 2) Exceptions (Now with Content) ---\n",
        "        if stage2_exception_rows:\n",
        "            print(f\"\\nFound {len(stage2_exception_rows)} final exceptions after Stage 2 regex parsing.\")\n",
        "            exceptions_df_s2 = pd.DataFrame(stage2_exception_rows)\n",
        "            # Reorder columns if content was successfully added\n",
        "            if not test_content_map: # Check if map creation failed\n",
        "                 exceptions_df_s2 = exceptions_df_s2[['id', 'raw_prediction']]\n",
        "                 print(\"Warning: Saving exceptions without 'content' column as map creation failed.\")\n",
        "            else:\n",
        "                 exceptions_df_s2 = exceptions_df_s2[['id', 'content', 'raw_prediction']]\n",
        "            print(f\"Saving final exceptions to: {stage2_exceptions_output_path}\")\n",
        "            exceptions_df_s2.to_csv(stage2_exceptions_output_path, index=False)\n",
        "            print(\"Final exceptions saved.\")\n",
        "        else:\n",
        "            print(\"\\nNo remaining exceptions after Stage 2.\")\n",
        "\n",
        "\n",
        "        # --- Combine Good Rows from Stage 1 and Stage 2 ---\n",
        "        print(\"\\nCombining results for final submission...\")\n",
        "        all_processed_rows_list = []\n",
        "        # ... (Load good_df_s1 as before) ...\n",
        "        if os.path.exists(stage1_good_rows_path):\n",
        "             try:\n",
        "                 good_df_s1 = pd.read_csv(stage1_good_rows_path)\n",
        "                 all_processed_rows_list.append(good_df_s1)\n",
        "                 print(f\"Loaded {len(good_df_s1)} rows from Stage 1 ({stage1_good_rows_path}).\")\n",
        "             except Exception as e:\n",
        "                 print(f\"Warning: Could not load good rows from Stage 1 file ({stage1_good_rows_path}): {e}\")\n",
        "        else:\n",
        "             print(f\"Warning: Stage 1 good rows file not found at {stage1_good_rows_path}.\")\n",
        "        # ... (Add stage2_good_rows as before) ...\n",
        "        if stage2_good_rows:\n",
        "            good_df_s2 = pd.DataFrame(stage2_good_rows)\n",
        "            final_columns = ['id'] + SUBMISSION_LABELS\n",
        "            good_df_s2 = good_df_s2.reindex(columns=final_columns, fill_value=0)\n",
        "            all_processed_rows_list.append(good_df_s2)\n",
        "            print(f\"Recovered {len(good_df_s2)} rows in Stage 2.\")\n",
        "\n",
        "        # ... (Concatenate, ensure uniqueness, reindex, save final submission as before) ...\n",
        "        if all_processed_rows_list:\n",
        "            final_submission_df = pd.concat(all_processed_rows_list, ignore_index=True)\n",
        "            final_submission_df = final_submission_df.drop_duplicates(subset=['id'], keep='first')\n",
        "            print(f\"Total unique rows in final submission: {len(final_submission_df)}\")\n",
        "            final_submission_df = final_submission_df.reindex(columns=['id'] + SUBMISSION_LABELS, fill_value=0)\n",
        "            print(f\"Saving final combined submission file to: {final_submission_path}\")\n",
        "            final_submission_df.to_csv(final_submission_path, index=False)\n",
        "            print(\"\\nFinal submission file head:\")\n",
        "            print(final_submission_df.head())\n",
        "            # ... (Optional verification count, compare against test_content_map size) ...\n",
        "            if test_content_map and len(test_content_map) != len(final_submission_df):\n",
        "                 print(f\"\\nWarning: Final submission row count ({len(final_submission_df)}) does not match original test set size ({len(test_content_map)}). Check for dropped IDs.\")\n",
        "            elif test_content_map:\n",
        "                 print(f\"\\nFinal submission row count ({len(final_submission_df)}) matches original test set size.\")\n",
        "\n",
        "        else:\n",
        "            print(\"ERROR: No rows processed successfully from any stage. Final submission file not created.\")\n",
        "\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: Input exceptions file not found at {exceptions_input_path}. Cannot run final parsing stage.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during final parsing stage: {e}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Skipping final parsing stage: Input exceptions file not found at {exceptions_input_path}.\")\n",
        "\n",
        "print(\"\\n--- Final Parsing Stage Finished ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8sIUWshvL7u",
        "outputId": "a7559101-2ca5-45f4-9dd9-9d5f3a0bb3bf"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating content map from 'test_dataset_raw' object...\n",
            "Created content map for 5736 test IDs.\n",
            "\n",
            "Loading Stage 1 exceptions from parsing_exceptions_fixed.csv...\n",
            "Loaded 98 exceptions for final processing.\n",
            "Using regex pattern: \\b(appeal_to_fear|bandwagon|cherry_picking|cliche|euphoria|fud|glittering_generalities|loaded_language|straw_man|whataboutism)\\b\n",
            "Parsing exceptions - Stage 2: Applying lenient regex...\n",
            "Processed S2 98/98... ID: 0369e166-88f0-4522-a127-51e542e3fa85 -> Status: RECOVERED, Found: {'whataboutism'}\n",
            "\n",
            "Found 3 final exceptions after Stage 2 regex parsing.\n",
            "Saving final exceptions to: parsing_exceptions_stage2_with_content.csv\n",
            "Final exceptions saved.\n",
            "\n",
            "Combining results for final submission...\n",
            "Loaded 5638 rows from Stage 1 (submission_part1_good_format.csv).\n",
            "Recovered 95 rows in Stage 2.\n",
            "Total unique rows in final submission: 5733\n",
            "Saving final combined submission file to: submission.csv\n",
            "\n",
            "Final submission file head:\n",
            "                                     id  appeal_to_fear  bandwagon  \\\n",
            "0                                    id               1          0   \n",
            "1  521cd2e8-dd9f-42c4-98ba-c0c8890ff1ba               0          0   \n",
            "2  9b2a61e4-d14e-4ff7-b304-e73d720319bf               0          0   \n",
            "3  f0f1c236-80a8-4d25-b30c-a420a39be632               0          0   \n",
            "4  31ea05ba-2c2b-4b84-aba7-f3cf6841b204               0          0   \n",
            "\n",
            "   cherry_picking  cliche  euphoria  fud  glittering_generalities  \\\n",
            "0               0       0         0    0                        0   \n",
            "1               0       1         0    0                        0   \n",
            "2               0       0         0    0                        0   \n",
            "3               0       0         0    1                        0   \n",
            "4               0       0         1    0                        0   \n",
            "\n",
            "   loaded_language  straw_man  whataboutism  \n",
            "0                0          0             0  \n",
            "1                1          0             0  \n",
            "2                1          0             0  \n",
            "3                1          0             0  \n",
            "4                1          0             0  \n",
            "\n",
            "Warning: Final submission row count (5733) does not match original test set size (5736). Check for dropped IDs.\n",
            "\n",
            "--- Final Parsing Stage Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Optional: Save Merged Model or GGUF ---\n",
        "\n",
        "# If you need the full merged model or GGUF format, add the saving code here.\n",
        "# Ensure 'model_for_inference' (the merged model) is available from the previous cell.\n",
        "\n",
        "save_merged_16bit = False # Set to True to save the full 16-bit merged model\n",
        "save_gguf = False       # Set to True to save in GGUF format\n",
        "\n",
        "if model_for_inference is not None:\n",
        "    # --- Save Merged Model (float16 / bfloat16) ---\n",
        "    if save_merged_16bit:\n",
        "        merged_path = os.path.join(output_dir, \"final_merged_16bit\")\n",
        "        print(f\"\\nSaving merged 16-bit model to {merged_path}...\")\n",
        "        try:\n",
        "            model_for_inference.save_pretrained(merged_path)\n",
        "            tokenizer_for_inference.save_pretrained(merged_path)\n",
        "            print(\"Merged model saved.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving merged model: {e}\")\n",
        "\n",
        "    # --- Save GGUF Model ---\n",
        "    if save_gguf:\n",
        "        # Define quantization type for GGUF (e.g., \"Q8_0\", \"Q4_K_M\", \"F16\")\n",
        "        # Note: Unsloth's GGUF saving might have specific supported types. Check their docs.\n",
        "        gguf_quant_type = \"Q8_0\"\n",
        "        gguf_path = os.path.join(output_dir, f\"final_model_{gguf_quant_type}.gguf\")\n",
        "        print(f\"\\nSaving GGUF model ({gguf_quant_type}) to {gguf_path}...\")\n",
        "        try:\n",
        "            # Use the correct method from Unsloth/FastModel if available, or PeftModel's method\n",
        "            # Assuming 'model_for_inference' is the merged PeftModel/FastModel object\n",
        "             model_for_inference.save_pretrained_gguf(\n",
        "                 output_dir, # Specifies the base name/directory\n",
        "                 tokenizer_for_inference,\n",
        "                 quantization_type = gguf_quant_type\n",
        "             )\n",
        "             # Rename the default gguf file if needed\n",
        "             default_gguf_name = os.path.join(output_dir, \"ggml-model-f16.gguf\") # Adjust if default name differs\n",
        "             quantized_gguf_name = os.path.join(output_dir, f\"ggml-model-{gguf_quant_type}.gguf\")\n",
        "             if os.path.exists(quantized_gguf_name):\n",
        "                 os.rename(quantized_gguf_name, gguf_path)\n",
        "                 print(f\"GGUF model saved as {gguf_path}\")\n",
        "             elif os.path.exists(default_gguf_name) and gguf_quant_type.upper() == \"F16\":\n",
        "                 os.rename(default_gguf_name, gguf_path)\n",
        "                 print(f\"GGUF model saved as {gguf_path}\")\n",
        "             else:\n",
        "                 print(f\"GGUF file with expected name not found after save attempt.\")\n",
        "\n",
        "        except AttributeError:\n",
        "             print(\"`save_pretrained_gguf` method not found on the model object.\")\n",
        "             print(\"Ensure you are using a recent version of Unsloth/Transformers that supports it, or use llama.cpp conversion tools.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving GGUF model: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping optional saving: Inference model not available.\")"
      ],
      "metadata": {
        "id": "Q9i0Ky3Rbwzg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}